<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cui的个人博客</title>
  <subtitle>不要让希望变成了失望</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://chr10003566.github.io./"/>
  <updated>2021-04-27T14:39:56.563Z</updated>
  <id>http://chr10003566.github.io./</id>
  
  <author>
    <name>Cui</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ATSS论文阅读笔记</title>
    <link href="http://chr10003566.github.io./2021/04/27/ATSS%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2021/04/27/ATSS论文阅读笔记/</id>
    <published>2021-04-27T14:17:58.000Z</published>
    <updated>2021-04-27T14:39:56.563Z</updated>
    
    <content type="html">&lt;h1 id=&quot;ATSS论文阅读笔记&quot;&gt;&lt;a href=&quot;#ATSS论文阅读笔记&quot; class=&quot;headerlink&quot; title=&quot;ATSS论文阅读笔记&quot;&gt;&lt;/a&gt;ATSS论文阅读笔记&lt;/h1&gt;&lt;p&gt;论文名称：&lt;strong&gt;Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;论文链接：&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02424&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1912.02424&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;代码地址：&lt;strong&gt;&lt;a href=&quot;https://github.com/sfzhang15/ATSS&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/sfzhang15/ATSS&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;1、Anchor-based方法和Anchor-free的方法最重要的差异就是在&lt;strong&gt;如何定义正负训练样本&lt;/strong&gt;;&lt;/p&gt;
&lt;p&gt;2、ATSS（&lt;em&gt;Adaptive Training Sample Selection&lt;/em&gt;）根据目标的统计分布自动调休安正负样本，可以明显地缩短anchor-free与anchor-based之间的差距。&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;近些年，&lt;strong&gt;Anchor-free的方法之所以被关注到是因为FPN与Focal Loss这两种方法的出现&lt;/strong&gt;，这里简单说明下我个人的看法，FPN能够输出多个层级的特征，允许不同尺度的物体在不同层级的特征预测得到，如果没有FPN，想像一下如何使用Anchor free的方法，我能想到的就是YOLOv1，如果object的中心落在特征图网格内，则由该网格负责预测GT框，YOLOv1的缺点大家也很清楚，不同物体落入同一个网格的话，那么只能预测其中一个，另一个则无法预测。且每幅图片只能预测98个候选框，则显然是不够的。但有了FPN，上面两个缺点则可以较好地解决，不同尺度的物体落入的特征层级不同，则可以在不同层级检测出两个可能落入同一个网格的物体。另外多层级特征也就可以预测更多的候选框。也就可以提升模型的召回率。再来就是Focal loss，Focal loss告知在Dense prediction中正负样本的选取有多么重要，而Anchor free则是标准的不能再标准的Dense prediction了。因此说FPN与Focal loss的出现才让Anchor free算法重新受到了广泛的关注。&lt;/p&gt;
&lt;p&gt;目前主流的Anchor-free的方法可以分成两种，1）通过定位几个预定的关键点，来确定物体的空间位置，这类方法算作keypoint-based methods，代表算法CornerNet和ExtremeNet。2）通过物体中心点以及中心点距离四条边界的距离，这类方法算作center-based methods，代表算法CenterNet，Foveabox，这些Anchor-free的检测器抛弃了与anchors相关的超参数，并且取得了与anchor-based方法想接近的效果。作者这里拿FCOS与RetinaNet作比较，探究造成anchor free的方法与anchor based的方法差异原因。&lt;/p&gt;
&lt;p&gt;FCOS与RetinaNet的主要差异：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RetinaNet在特征图上每个点会叠加多个不同长宽比的anchor，而FCOS则是在每个位置叠加一个anchor的中心点；&lt;/li&gt;
&lt;li&gt;RetinaNet是通过IoU来定义正负样本的，而FCOS则是通过anchor的中心点与GroundTruth的中心点的距离与尺度定义正负样本的。&lt;/li&gt;
&lt;li&gt;RetinaNet的回归策略是预测预设定的anchor box与物体GroundTruth的偏移量，而FCOS则是预测anchor中心点与GroundTruth中心的偏移量，以及点与各个边界的距离。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者通过实验，证实&lt;strong&gt;差异（2）&lt;/strong&gt;是导致两个检测器效果差异的主要原因。并证实了在同一个位置堆叠多个不同的anchor是不必要的操作，因此作者提出了ATSS，根据物体的统计分布来自动地挑选正负样本。最终在COCO上取得了SOTA的结果。&lt;/p&gt;
&lt;h2 id=&quot;相关工作&quot;&gt;&lt;a href=&quot;#相关工作&quot; class=&quot;headerlink&quot; title=&quot;相关工作&quot;&gt;&lt;/a&gt;相关工作&lt;/h2&gt;&lt;p&gt;本文的相关工作总结的比较到位，故在此整理下：&lt;/p&gt;
&lt;h3 id=&quot;Two-stage-method：&quot;&gt;&lt;a href=&quot;#Two-stage-method：&quot; class=&quot;headerlink&quot; title=&quot;Two stage method：&quot;&gt;&lt;/a&gt;Two stage method：&lt;/h3&gt;&lt;p&gt;最具代表的：Faster R-CNN&lt;/p&gt;
&lt;p&gt;结构重新设计：R-FCN，TridentNet，Cascade R-CNN，ME R-CNN，MS-R-CNN&lt;/p&gt;
&lt;p&gt;引入上下文信息和注意力机制（基本没看过）：Inside-outside net[2]，Thundernet 等等&lt;/p&gt;
&lt;p&gt;多尺度训练：SNIP和AutoFocus&lt;/p&gt;
&lt;p&gt;特征融合：FPN、c&lt;/p&gt;
&lt;p&gt;更好地候选框与解决不平衡问题：Libra R-CNN&lt;/p&gt;
&lt;h3 id=&quot;One-stage-method&quot;&gt;&lt;a href=&quot;#One-stage-method&quot; class=&quot;headerlink&quot; title=&quot;One stage method&quot;&gt;&lt;/a&gt;One stage method&lt;/h3&gt;&lt;p&gt;最具代表性的：SSD&lt;/p&gt;
&lt;p&gt;融合不同层上下文信息：RON，DSSD，STDN&lt;/p&gt;
&lt;p&gt;从头训练：DSOD，ScratchDet&lt;/p&gt;
&lt;p&gt;新的loss：Focal loss，ap-loss&lt;/p&gt;
&lt;p&gt;anchor提炼：RefineDet， Freeanchor&lt;/p&gt;
&lt;p&gt;结构重新设计：PFPNet&lt;/p&gt;
&lt;p&gt;特征增强与对齐（基本没看过）：&lt;/p&gt;
&lt;h3 id=&quot;keypoint-based-method&quot;&gt;&lt;a href=&quot;#keypoint-based-method&quot; class=&quot;headerlink&quot; title=&quot;keypoint-based method&quot;&gt;&lt;/a&gt;keypoint-based method&lt;/h3&gt;&lt;p&gt;CornerNet、Grid R-CNN、ExtremeNet、RepPoints&lt;/p&gt;
&lt;h3 id=&quot;Center-based-method&quot;&gt;&lt;a href=&quot;#Center-based-method&quot; class=&quot;headerlink&quot; title=&quot;Center-based method&quot;&gt;&lt;/a&gt;Center-based method&lt;/h3&gt;&lt;p&gt;YOLO、DenseBox、GA-RPN、FSAF、FCOS、CSP、FoveaBox&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h2&gt;&lt;p&gt;作者通过设计对比实验，深究FCOS与RetinaNet两者的差异。首先控制差异（1），将RetinaNet每一层特征只分配一个anchor，anchor的大小为8&lt;em&gt;S&lt;/em&gt;，&lt;em&gt;S&lt;/em&gt;表示当前特征层下采样的倍数。此外作者还把FCOS一些提点方法补充到了RetinaNet上了，实验结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60881bd7d1a9ae528f8f7667.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;发现仍然有0.8%个点的差异，那这个部分差异从哪里来呢？只能来自剩下的两个差异了，差异（2）说白了就是分类子任务，差异（3）则是回归子任务。&lt;/p&gt;
&lt;h3 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h3&gt;&lt;p&gt;RetinaNet是通过IoU阈值来挑选正样本的，GT框与候选框的$IoU&amp;gt;\Theta&lt;em&gt;{p}$则认为该候选框为正样本而则$IoU&amp;lt;\Theta&lt;/em&gt;{n}$则认为该候选框为负样本，其他样本则忽略。&lt;/p&gt;
&lt;p&gt;而FCOS通过空间和尺度的约束来划分正负样本，如果anchor points在GT框内，则该point为候选点；然后通过point与各边界距离的最大值是否在该层级特征预设定的范围内，最终决定该point是否为正样本还是负样本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60881c0fd1a9ae528f923997.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者通过以下的实验，两种挑选正负样本的方式，确实是导致FCOS与RetinaNet的差异的原因之一，FCOS使用IoU策略，效果下降。与RetinaNet基本一致，而RetinaNet使用空间和尺度的约束来挑选正负样本，效果提升至37.8%，与FCOS一致了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60881c2ed1a9ae528f93c081.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PS：&lt;/strong&gt;这里的实验没有太理解，FCOS如何接入IoU策略的，在没有anchor box的情况下，哪里来的IoU？&lt;/p&gt;
&lt;h3 id=&quot;回归&quot;&gt;&lt;a href=&quot;#回归&quot; class=&quot;headerlink&quot; title=&quot;回归&quot;&gt;&lt;/a&gt;回归&lt;/h3&gt;&lt;p&gt;RetinaNet是通过计算四个偏移量，anchor中心点与GT框中心点的偏移量与高宽的偏移量，而FCOS回归的则是中心点与四条边界的距离；在下图展示的十分明显。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60881c54d1a9ae528f95a153.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;通过实验发现，无论是回归点还是回归候选框，两者的差异都不是很大，&lt;/p&gt;
&lt;p&gt;说明差异（3）不是造成anchor-based与anchor-free方法效果差异的主要原因。进一步证实了差异（2）正负样本的挑选才是anchor-based与anchor-free方法效果差异的主要原因！&lt;/p&gt;
&lt;h2 id=&quot;Adaptive-Training-Sample-Selection&quot;&gt;&lt;a href=&quot;#Adaptive-Training-Sample-Selection&quot; class=&quot;headerlink&quot; title=&quot;Adaptive Training Sample Selection&quot;&gt;&lt;/a&gt;Adaptive Training Sample Selection&lt;/h2&gt;&lt;p&gt;ATSS其实并没有很复杂，也是大家日常在挑选训练集的时候，可能都会用到的方法，只是这里用到了挑选正负样本上。ATSS算法步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先为每一个GT box挑选 中心点与其最近的K个anchor box（熟不熟悉，YOLOF的k-近邻算法 :cry: ）作为候选框.&lt;/li&gt;
&lt;li&gt;计算候选框与GT box的IoU，计算出其均值$m&lt;em&gt;{g}$和方差$v&lt;/em&gt;{g}$，再计算IoU的阈值$t&lt;em&gt;{g} = m&lt;/em&gt;{g} + v_{g}$&lt;/li&gt;
&lt;li&gt;最后挑选候选框与GT box的IoU大于阈值$t_{g}$的作为最后的正样本，当然对正样本有些约束，要求正样本中心点必须在GT box内&lt;/li&gt;
&lt;li&gt;如果一个anchor box被分配到多个GT box，选择最大IoU的那个 作为他的真值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60881c9ad1a9ae528f992eab.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后，在看下该策略的效果，普遍能涨2~3%个点。还是有些用的。&lt;/p&gt;
&lt;h3 id=&quot;感想&quot;&gt;&lt;a href=&quot;#感想&quot; class=&quot;headerlink&quot; title=&quot;感想&quot;&gt;&lt;/a&gt;感想&lt;/h3&gt;&lt;p&gt;本篇文章，最值得借鉴的是FCOS与RetinaNet差异性对比的实验设计，控制变量，逐步分析。确实考虑的足够细致，值的学习！而后续的ATSS不是很复杂，但作为涨点的技巧也是可以学习的。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ATSS论文阅读笔记&quot;&gt;&lt;a href=&quot;#ATSS论文阅读笔记&quot; class=&quot;headerlink&quot; title=&quot;ATSS论文阅读笔记&quot;&gt;&lt;/a&gt;ATSS论文阅读笔记&lt;/h1&gt;&lt;p&gt;论文名称：&lt;strong&gt;Bridging the Gap Between 
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>YOLOF论文阅读笔记</title>
    <link href="http://chr10003566.github.io./2021/04/22/YOLOF%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2021/04/22/YOLOF论文阅读笔记/</id>
    <published>2021-04-22T15:05:36.000Z</published>
    <updated>2021-04-22T16:31:20.643Z</updated>
    
    <content type="html">&lt;h1 id=&quot;YOLOF论文阅读&quot;&gt;&lt;a href=&quot;#YOLOF论文阅读&quot; class=&quot;headerlink&quot; title=&quot;YOLOF论文阅读&quot;&gt;&lt;/a&gt;YOLOF论文阅读&lt;/h1&gt;&lt;p&gt;论文名称：&lt;strong&gt;You Only Look One-level Feature&lt;/strong&gt;&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/2103.09460.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/2103.09460.pdf&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/megvii-model/YOLOF&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/megvii-model/YOLOF&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;1、 作者探究了FPN之所以能提升检测效果的原因：主要是因为分治的思想而非多尺度特征融合；&lt;br&gt;2、 作者提出一种替换FPN复杂的特征金字塔的方案，仅使用一个层级的特征进行检测，提出了YOLOF，其中两个重要组件&lt;strong&gt;Dilated Encoder&lt;/strong&gt;和&lt;strong&gt;Uniform Matching&lt;/strong&gt;；&lt;br&gt;3、 YOLOF取得了不错的效果，速度和精度权衡的效果比YOLOv4还要好。&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;FPN已经成为现在主流检测框架必备的组件了，FPN之所以有效源自于以下两个优点：1）多尺度特征融合，融合了高层的语义特征与低分辨率的图像特征，是特征更具有代表性。 2）分而治之的思想，将检测问题按尺度进行拆分，不同尺度的图像由不同的特征去预测。作者设计实验发现，&lt;strong&gt;主要是因为分而治之的思想，使得FPN如此高效&lt;/strong&gt;。作者设计了以下实验：（PS：可以借鉴下作者设计实验的思路以及写法）&lt;br&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60819949d1a9ae528f8f9762.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者设计了四组实验，通过对比发现MiMO仅仅比SiMO提升不到1%，也就说明特征融合带来的效果也就仅仅只有1%，而MiSO与MiMO却有12%的差异。通过这组对比试验就发现FPN能如此高效的主要原因是因为分而治之的思想。&lt;/p&gt;
&lt;p&gt;但是呢，&lt;strong&gt;分而治之的思想除了带来了检测效果的提升，同时也带来了内存的负担，以及使得检测的结构变得复杂&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;作者同样设计了实验发现，仅仅使用单层级特征（即上图SiSO）其速度、模型容量等都远小于MiMO。因此作者希望能设计算法缩小SiSo与MiMo的检测效果的差异。&lt;br&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60819aeed1a9ae528fac147f.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者分析后，1）需要弥补SiSo的单层级特征相比于MiMo的多层级特征在特征尺度层级上的差别； 2）需要弥补因为单层级特征导致的，候选框不平衡的问题。&lt;/p&gt;
&lt;p&gt;因此作者提出了&lt;strong&gt;Dilated Encoder&lt;/strong&gt;解决单层级特征缺少多层级语义信息的问题，&lt;strong&gt;Uniform Matching&lt;/strong&gt;解决候选框不平衡的问题。&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h2&gt;&lt;h3 id=&quot;Dilated-Encoder&quot;&gt;&lt;a href=&quot;#Dilated-Encoder&quot; class=&quot;headerlink&quot; title=&quot;Dilated Encoder&quot;&gt;&lt;/a&gt;Dilated Encoder&lt;/h3&gt;&lt;p&gt;作者利用C5（下采样32倍）作为单层级的特征，发现C5特征的感受野仅仅只能覆盖一部分的尺度范围的物体（如下图（a）），这就导致了糟糕的检测效果。而要使得其特征能识别更多的物体，就要使其特征的感受野能匹配更多物体的尺寸，因此作者需要想办法扩充输出特征的感受野范围。&lt;br&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60819da5d1a9ae528fd88e4f.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;那就是利用空洞卷积，在TridentNet等文献中已经证明了空洞卷积在检测上的有效性。因此作者设计了以下的结构，使输出的特征包含更多的感受野范围。&lt;br&gt;&lt;img src=&quot;https://img.imgdb.cn/item/60819e0cd1a9ae528fded085.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Uniform-Matching&quot;&gt;&lt;a href=&quot;#Uniform-Matching&quot; class=&quot;headerlink&quot; title=&quot;Uniform Matching&quot;&gt;&lt;/a&gt;Uniform Matching&lt;/h3&gt;&lt;p&gt;在目标检测中，如何定义positive anchor是十分重要的，通常使用max-IoU matching，即anchor与Ground truth的IoU大于0.5，就认为这个anchor是positive anchor。&lt;/p&gt;
&lt;p&gt;上面说过了，作者使用了C5的单层级特征来进行检测，使用高层级特征必然就会更倾向于预测大物体（感受野比较大，则会更倾向于预测大物体），因此也就会导致如果使用Max-IoU match的话，positive anchor肯定大部分都是大物体，小物体的positive anchor会很少，因此存在严重的不平衡问题。（就是小物体的检出率会严重下降）。为了保持大物体与小物体anchor数目的差距，作者使用了k-nearest算法，把每个Groundtruth附近的k个anchor作为正样本，这样不管物体尺寸是多少，其positive anchor的数量是一致的。并且作者做了一些调整，对于选出来的anchor， 如果IoU &amp;gt; 0.7 那么不能算作 negative anchors 如果IoU  &amp;lt; 0.15，不能作为positive anchor。&lt;/p&gt;
&lt;p&gt;最后实验结果如下：&lt;br&gt;&lt;img src=&quot;https://img.imgdb.cn/item/6081a141d1a9ae528f0a78d5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;##感想&lt;br&gt;本文的行文方式，着实让我有些摸不着头脑，前面探究FPN表现如此出色的原因是因为分治的思想，后续并没有按照分治的思想来做。&lt;/p&gt;
&lt;p&gt;再有就是大家都在讨论的，C5层的特征感受野已经足够大了，文章还在继续增大其输出特征的感受野，这明显会牺牲小物体的检出，虽然文章在最后的实验部分提到，在大物体检测上提升了3.3%，小物体上下降了3.1%。但小物体的检测不应该才是重点嘛。&lt;/p&gt;
&lt;p&gt;文章的两个创新点，1）利用空洞卷积提升输出的单层级特征感受野的范围 而在TridentNet里利用分治的思想，不同特征不同感受野来预测效果明显要更佳。 2）利用k-近邻来筛选anchor。这些都不能算上出色的创新点。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;YOLOF论文阅读&quot;&gt;&lt;a href=&quot;#YOLOF论文阅读&quot; class=&quot;headerlink&quot; title=&quot;YOLOF论文阅读&quot;&gt;&lt;/a&gt;YOLOF论文阅读&lt;/h1&gt;&lt;p&gt;论文名称：&lt;strong&gt;You Only Look One-level Featu
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>Libra R-CNN论文阅读笔记</title>
    <link href="http://chr10003566.github.io./2021/04/18/Libra-R-CNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2021/04/18/Libra-R-CNN论文阅读笔记/</id>
    <published>2021-04-18T15:28:48.000Z</published>
    <updated>2021-04-18T15:50:35.708Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Libra-R-CNN论文阅读笔记&quot;&gt;&lt;a href=&quot;#Libra-R-CNN论文阅读笔记&quot; class=&quot;headerlink&quot; title=&quot;Libra R-CNN论文阅读笔记&quot;&gt;&lt;/a&gt;Libra R-CNN论文阅读笔记&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;1、检测器的性能所限于训练过程的不平衡问题，作者主要从三个层面解决训练过程的不平衡问题–&lt;strong&gt;sample level，feature level，objective level。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2、&lt;strong&gt;IoU balanced&lt;/strong&gt;对应sample level的解决方案，&lt;strong&gt;balanced feature pyrmaid&lt;/strong&gt;对应feature level的解决方案，balanced L1 loss对应Objective level的解决方案。&lt;/p&gt;
&lt;p&gt;ps：这种结构的文章看起来还是舒服的，写起来应该也很舒服吧，哈哈哈。&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;无论是单阶段还是两阶段的目标检测算法，其训练的整体流程大致相同：sampling regions -&amp;gt;extracting feature-&amp;gt;recoginizing the categories and refining the location.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/607c3aa88322e6675c89d274.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt; 那么在这个流程下，就有三个决定性的因素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选取的候选区域是否具有代表性？往往采用Random Sampling的方式来挖掘训练样本，而这样通常会导致Easy example的比例远大于Hard example，导致了不平衡（上图a）&lt;/li&gt;
&lt;li&gt;提取的特征是否足够鲁棒，包含了所有信息？往往High level feature缺少边缘等图像层级的特征，而Low level feature缺少语义层级的特征。（上图b）&lt;/li&gt;
&lt;li&gt;设计的目标函数是否最优？目标检测的损失函数往往由分类损失与定位损失共同组成，设计的损失函数可能会更偏向某一部分，而导致了不平衡。（上图c）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先让我们来看下针对上述问题，过去有哪些解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、针对Sample level imbalance&lt;/strong&gt;：OHEM它能够让模型更专注Hard example，但是它对噪声标签很敏感，并且有相当大的内存和计算成本。而大名鼎鼎的Focal loss，在两阶段的目标检测算法上表现并不理想（RoI的存在）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、针对Feature level imbalance：&lt;/strong&gt;FPN这种方式的特征串联，仅仅使集成后的特征更加关注相邻分辨率的特征，而却很少关注其他分辨率的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、针对Objective level imbalance：&lt;/strong&gt;目标检测需要很好地权衡好分类损失和定位损失，如果没有很好的平衡好，有可能导致Easy example的小梯度被Hard example的大梯度给淹没了，导致模型学“歪”了。&lt;/p&gt;
&lt;p&gt;本文的分别提出了三个方案来解决上述的三个&lt;strong&gt;Imbalance&lt;/strong&gt;问题。&lt;/p&gt;
&lt;p&gt;1、 IoU-balanced Sampling -&amp;gt;Sample level imbalance&lt;/p&gt;
&lt;p&gt;2、Balanced Feature Pyramid-&amp;gt;Feature level imbalance&lt;/p&gt;
&lt;p&gt;3、Balanced L1 Loss-&amp;gt;Objective level imbalance&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h2&gt;&lt;h4 id=&quot;IoU-balanced-Sampling&quot;&gt;&lt;a href=&quot;#IoU-balanced-Sampling&quot; class=&quot;headerlink&quot; title=&quot;IoU-balanced Sampling&quot;&gt;&lt;/a&gt;IoU-balanced Sampling&lt;/h4&gt;&lt;p&gt;作者发现超过60%的Hard example与真值框有超过0.05的IoU，而Random sample仅仅提供了30%的IoU大于0.05的样本。极度的样本分布不平衡导致了众多的Hard example被淹没在了Easy example中。因此作者提出了以下的方案：&lt;/p&gt;
&lt;p&gt;假设我们要从M个相关候选样本中选出N个负样本，则&lt;strong&gt;根据IoU的值，将采样区间换分成K个bin，然后N要求负样本在每个格子中使均匀分布，然后对其进行均匀采样&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&quot;Balanced-Feature-Pyramid&quot;&gt;&lt;a href=&quot;#Balanced-Feature-Pyramid&quot; class=&quot;headerlink&quot; title=&quot;Balanced Feature Pyramid&quot;&gt;&lt;/a&gt;Balanced Feature Pyramid&lt;/h4&gt;&lt;p&gt;不同于FPN，作者通过将不同深度的特征融合来增强融合特征后各个层级特征的信息。主要包含以下四个步骤：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;rescaling，integrating，refining和strengthening&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/607c45898322e6675ca3b265.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者同样使用到了FPN，将FPN的各层特征选择层级处于中间尺寸的（如上图C4），通过插值或者最大池化操作，将其他层级的特征&lt;strong&gt;resacle&lt;/strong&gt;到该层级的尺寸，然后将rescale后的各层级特征&lt;strong&gt;integrate&lt;/strong&gt;起来（直接通过取均值的方式）。之后在通过卷积层或者non-local module对特征进行&lt;strong&gt;refine&lt;/strong&gt;，最后像FPN一样输出多个层级的特征。（Question：1）strengthen步骤在哪里啊，和refine一起的吗？ 2）为何要像FPN一样多层级输出呢，直接用refine后特征效果如何呢？）&lt;/p&gt;
&lt;h4 id=&quot;Balanced-L1-Loss&quot;&gt;&lt;a href=&quot;#Balanced-L1-Loss&quot; class=&quot;headerlink&quot; title=&quot;Balanced L1 Loss&quot;&gt;&lt;/a&gt;Balanced L1 Loss&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/607c481e8322e6675ca965f2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Smooth L1 Loss我们都知道是大部分两阶段目标检测算法的定位损失，可以看到上图（a)，在regression error较小时，Smooth L1 loss的梯度是比较小的，regression error较小也就对应了那些定位相对比较准确的Easy example，其梯度也比较小。这样就有可能导致Easy example的小梯度被Hard example的大梯度给淹没了（ps：Focal loss不是说Easy example梯度虽然小，但是其量级大呀。）作者为了能够&lt;strong&gt;解决小梯度的Easy example梯度被淹没的情况，提出了Balanced L1 loss&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.imgdb.cn/item/607c4be98322e6675cb22e74.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者通过两个参数，$\alpha$ 和 $\gamma$两个参数来权衡。 当$\alpha$取越小的值时，它对Easy Example梯度回传的力度更大，但对Hard Example的梯度回传不会有影响。&lt;/p&gt;
&lt;h2 id=&quot;感想&quot;&gt;&lt;a href=&quot;#感想&quot; class=&quot;headerlink&quot; title=&quot;感想&quot;&gt;&lt;/a&gt;感想&lt;/h2&gt;&lt;p&gt;这篇文章看下来，提供了很多训练目标检测算法的trick，文章展现的效果是挺好的，但还是要结合实际的项目去实践。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Libra-R-CNN论文阅读笔记&quot;&gt;&lt;a href=&quot;#Libra-R-CNN论文阅读笔记&quot; class=&quot;headerlink&quot; title=&quot;Libra R-CNN论文阅读笔记&quot;&gt;&lt;/a&gt;Libra R-CNN论文阅读笔记&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>FSAF论文笔记</title>
    <link href="http://chr10003566.github.io./2020/03/04/FSAF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/03/04/FSAF论文笔记/</id>
    <published>2020-03-04T02:36:27.000Z</published>
    <updated>2020-03-04T02:36:27.992Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>FCOS论文笔记</title>
    <link href="http://chr10003566.github.io./2020/03/03/FCOS%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/03/03/FCOS论文笔记/</id>
    <published>2020-03-03T01:37:47.000Z</published>
    <updated>2020-03-03T06:48:38.022Z</updated>
    
    <content type="html">&lt;p&gt;论文：FCOS: Fully Convolutional One-Stage Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.01355&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1904.01355&lt;/a&gt;&lt;br&gt;论文代码：&lt;a href=&quot;https://github.com/tianzhi0549/FCOS&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/tianzhi0549/FCOS&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;首先作者按照“惯例”说明了基于Anchor based&lt;br&gt;检测器的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于Anchor的检测算法，检测效果受预选框尺寸和比例大小、以及检测框数量的影响很大&lt;/li&gt;
&lt;li&gt;Anchor的设置，如尺寸、比例固定，导致基于Anchor的检测算法在处理尺度变化大或者小目标的时候存在一些问题&lt;/li&gt;
&lt;li&gt;基于Anchor的检测算法，会导致正负样本不平衡&lt;/li&gt;
&lt;li&gt;在训练过程中，还需要候选框与GroundTruth的IoU这样复杂的计算过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;鉴于全卷积神经网络的dense prediction在分割、关键点检测等领域取得的成就，作者尝试讲全卷积神经网络运用到检测当中，对每个pixel进行预测。此前也不是没有通过对每个pixel进行密集预测的检测器（DenseBox、YOLOv1），但都存在不少的问题。对于DenseBox，它必须讲候选框裁剪到固定的尺寸，才能输入到网络中。因此其必须使用图像金字塔进行训练。而对于YOLOv1由于是对每个pixel预测两个候选框，其预测结果会严重遗漏候选框。在对pixel进行密集预测中，有一个问题必须要解决，即对于高度重合的候选框，同一个像素点要回归到哪一个候选框是难以节点的。比如图一中右图，对于同一个像素点，是回归到橙色框还是蓝色框，是pixel密集预测需要解决的问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5dbbe598271cb2b809e29e.png&quot; alt=&quot;图一&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Method&quot;&gt;&lt;a href=&quot;#Method&quot; class=&quot;headerlink&quot; title=&quot;Method&quot;&gt;&lt;/a&gt;Method&lt;/h2&gt;&lt;h3 id=&quot;Architecture&quot;&gt;&lt;a href=&quot;#Architecture&quot; class=&quot;headerlink&quot; title=&quot;Architecture&quot;&gt;&lt;/a&gt;Architecture&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5dcc4998271cb2b80db27b.png&quot; alt=&quot;图二&quot;&gt;&lt;/p&gt;
&lt;p&gt;FCOS的整体框架图如上图所示，同样的与FoveaBox一样，是在RetinaNet的架构上作修改。和FoveaBox一样将RetinaNet的输出的两个feature map的channel做了修改，输出的用于分类feature map通道数从C x A改成了C[C表示数据集的类别数，A表示RetinaNet设置的anchor数量]，而用于回归的feature map通道数则是从4 x A改成了4，完全摒弃了Anchor。其思路与FoveaBox一直，基于pixel的密集预测。对输出的feature map 每个点(x, y)预测一个4维的向量\(\boldsymbol{t}^{&lt;em&gt;}=\left(l^{&lt;/em&gt;}, t^{&lt;em&gt;}, r^{&lt;/em&gt;}, b^{&lt;em&gt;}\right)\)[见图一]，以及它的标签c∗，在训练过程通过将feature map该点的坐标映射会原图坐标\( \left(\left\lfloor\frac{s}{2}\right\rfloor+ x s,\left\lfloor\frac{s}{2}\right\rfloor+ y s\right) \)，其中s为下采样的倍数。若该点落在Groundtruth的框内，且预测的类别c&lt;/em&gt;与GroundTruth的标签c一直，则认为该点（x，y）是正样本，否则认为该点是负样本。如果同一个点（x,y）同时落入了多个Groundtruth区域，则该点应该选择最小的Groundtruth作为回归对象（即图一右图的情况，橙色的点应该回归到蓝色的框）。&lt;/p&gt;
&lt;h3 id=&quot;Loss-Function&quot;&gt;&lt;a href=&quot;#Loss-Function&quot; class=&quot;headerlink&quot; title=&quot;Loss Function&quot;&gt;&lt;/a&gt;Loss Function&lt;/h3&gt;&lt;p&gt;$$&lt;br&gt;\begin{aligned} L\left(\left{\boldsymbol{p}&lt;em&gt;{x, y}\right},\left{\boldsymbol{t}&lt;/em&gt;{x, y}\right}\right) &amp;amp;=\frac{1}{N&lt;em&gt;{\text {pos }}} \sum&lt;/em&gt;{x, y} L&lt;em&gt;{\text {cls }}\left(\boldsymbol{p}&lt;/em&gt;{x, y}, c&lt;em&gt;{x, y}^{*}\right) \ &amp;amp;+\frac{\lambda}{N&lt;/em&gt;{\text {pos }}} \sum&lt;em&gt;{x, y} \mathbb{1}&lt;/em&gt;{\left{c&lt;em&gt;{x, y}^{*}&amp;gt;0\right}} L&lt;/em&gt;{\text {reg }}\left(\boldsymbol{t}&lt;em&gt;{x, y}, \boldsymbol{t}&lt;/em&gt;{x, y}^{*}\right) \end{aligned}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;作者使用了上述公式作为损失函数，\(L&lt;em&gt;{cls}\)为Focal loss分类损失，\(L&lt;/em&gt;{reg}\)则是IoU loss，且与其他检测的损失函数一样，只有当该点被判断为正样本才计算回归损失。&lt;/p&gt;
&lt;h3 id=&quot;Inference&quot;&gt;&lt;a href=&quot;#Inference&quot; class=&quot;headerlink&quot; title=&quot;Inference&quot;&gt;&lt;/a&gt;Inference&lt;/h3&gt;&lt;p&gt;测试阶段就比较简单，对feature map上的每个piexl预测它的\(p&lt;em&gt;{xy}\)和\(t&lt;/em&gt;{xy}\)，然后选择\(p&lt;em&gt;{xy} &amp;gt; 0.05\)的点，将\(t&lt;/em&gt;{xy}\)通过下面的公式扩展成候选框&lt;br&gt;$$&lt;br&gt;\begin{aligned} l^{&lt;em&gt;} &amp;amp;=x-x_{0}^{(i)}, \quad t^{&lt;/em&gt;}=y-y&lt;em&gt;{0}^{(i)} \ r^{*} &amp;amp;=x&lt;/em&gt;{1}^{(i)}-x, \quad b^{*}=y_{1}^{(i)}-y \end{aligned}&lt;br&gt;$$&lt;/p&gt;
&lt;h3 id=&quot;Multi-Level&quot;&gt;&lt;a href=&quot;#Multi-Level&quot; class=&quot;headerlink&quot; title=&quot;Multi Level&quot;&gt;&lt;/a&gt;Multi Level&lt;/h3&gt;&lt;p&gt;作者同样使用了FPN来做一个multi level的多层级预测，之所以这么考虑是基于以下两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果像分类一样仅使用下采样倍数最大的一层做检测（感受野过大），会导致大部分小物体漏检&lt;/li&gt;
&lt;li&gt;若不使用multi level，对于高度重叠的矩形框不太好处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里作者提出了一个观点，大部分重叠的框，都不在同一个尺寸的。只有较少部分是同样尺寸的候选框相互重叠。针对不同尺寸的重叠候选框，利用multi level可以比较好的解决该情况。即不同尺寸的通过不同层级的feature map来回归获得，而针对相同尺寸的候选框重叠，则选择那个面积更小的作为回归。&lt;/p&gt;
&lt;p&gt;那作者是如何判断哪个尺寸的候选框由哪个层级的feature map来预测呢，此处作者并未像FoveaBox和FPN一样，而是直接利用回归的结果来判断，即\(m&lt;em&gt;{i-1}&amp;lt;&lt;br&gt;\max \left(l^{&lt;em&gt;}, t^{&lt;/em&gt;}, r^{&lt;em&gt;}, b^{&lt;/em&gt;}\right)&amp;lt;m&lt;/em&gt;{i}\)。&lt;br&gt;在文章中\(m&lt;em&gt;{2}\),\(m&lt;/em&gt;{3}\),\(m&lt;em&gt;{4}\),\(m&lt;/em&gt;{5}\),\(m&lt;em&gt;{6}\),\(m&lt;/em&gt;{7}\)被设置成 0, 64, 128, 256, 512 and ∞.&lt;/p&gt;
&lt;h3 id=&quot;Center-ness-for-FCOS&quot;&gt;&lt;a href=&quot;#Center-ness-for-FCOS&quot; class=&quot;headerlink&quot; title=&quot;Center-ness for FCOS&quot;&gt;&lt;/a&gt;Center-ness for FCOS&lt;/h3&gt;&lt;p&gt;在使用了上述技巧后，作者发现实验得到的结果仍然未达到RetinaNet的实验效果，在分析了实验结果后，作者发现部分误检框离真实框的中心点距离较大，也就是说这部分其实是错得比较离谱的误检，因此作者用一种比较简单的方式减少这种误检，这个方法的核心就是将分类支路的输出乘以一个权重图得到最终的分类置信度。&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5df02898271cb2b8170a2e.png&quot; alt=&quot;图三&quot;&gt;&lt;br&gt;从图三可以看到，光凭classification得到的score（即前面的\(p_{xy}\))会出现很多的误检框，这些误检框大部分是离中心点距离较大，因此加入了一个分类置信度，输入则是回归支路的得到的结果\(\boldsymbol{t}^{&lt;em&gt;}=\left(l^{&lt;/em&gt;}, t^{&lt;em&gt;}, r^{&lt;/em&gt;}, b^{&lt;em&gt;}\right)\)，通过以下公式得到。&lt;br&gt;$$&lt;br&gt;centerness  ^{&lt;/em&gt;}=\sqrt{\frac{\min \left(l^{&lt;em&gt;}, r^{&lt;/em&gt;}\right)}{\max \left(l^{&lt;em&gt;}, r^{&lt;/em&gt;}\right)} \times \frac{\min \left(t^{&lt;em&gt;}, b^{&lt;/em&gt;}\right)}{\max \left(t^{&lt;em&gt;}, b^{&lt;/em&gt;}\right)}}&lt;br&gt;$$&lt;br&gt;通过公式，可以比较清楚的了解到，当偏离中心点较大时，centerness的值则较小，因此最后分类得到的置信度也会变小。从而减少了误检框的个数。&lt;/p&gt;
&lt;h3 id=&quot;Experiment&quot;&gt;&lt;a href=&quot;#Experiment&quot; class=&quot;headerlink&quot; title=&quot;Experiment&quot;&gt;&lt;/a&gt;Experiment&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5df2c498271cb2b817c3d5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Summary&quot;&gt;&lt;a href=&quot;#Summary&quot; class=&quot;headerlink&quot; title=&quot;Summary&quot;&gt;&lt;/a&gt;Summary&lt;/h3&gt;&lt;p&gt;在近期阅读了一些Anchor free的方法后，做一些总结；&lt;/p&gt;
&lt;p&gt;Anchor free的方法大致可以分为：&lt;br&gt;1）基于关键点检测沿升到目标检测，其主要解决的问题在于&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要寻找哪些关键点？&lt;br&gt;CornerNet 需要找到左上角点与右下角点，CenterNet（华为）同样需要找到左上角点与右下角点以及中心点，CenterNet（Objects as points）只需要中心点，ExtremeNet则是找到4个极点与中心点&lt;/li&gt;
&lt;li&gt;关键点如何转换成矩形框？&lt;br&gt;CornerNet、CenterNet（华为）通过计算左上角点与右下角点Embedding Vector的距离，&lt;br&gt;CenterNet（Objects as points）通过中心点与回归出来的长宽构成矩形框，ExtremeNet则是通过极点的组合是否包括中心点来判断是否组成矩形框。&lt;/li&gt;
&lt;li&gt;后处理过程&lt;br&gt;通过一系列操作，刨除误检框，CenterNet（华为）通过中心点判断其框是否属于误检框等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2）基于feature map的每个pixel进行dense prediction，其主要解决的问题在于 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何定义正负样本点？&lt;br&gt;FoveaBox 通过设置参数，选取Groundtruth中心点某个范围（参数1）内的点作为正样本点，某个范围外（参数2）的点全部是负样本点，FCOS则是落入Grondtruth中的点均为正样本点，其余的点都为负样本。（FCOS认为这样增加了正样本的数量，而FoveaBox则认为应该选择更精细的点作为正样本）&lt;/li&gt;
&lt;li&gt;如何解决同一个点回归不同的矩形框？&lt;br&gt;都是采用多层级预测（FPN）来实现，只是两者判断不同尺寸的物体应该由哪一层的特征图来预测的方式不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Note&quot;&gt;&lt;a href=&quot;#Note&quot; class=&quot;headerlink&quot; title=&quot;Note&quot;&gt;&lt;/a&gt;Note&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;作者在将预测点的回归值，转换成候选框的时候（x，y)似乎还是feature map上的坐标，而非原图的坐标，是作者的笔误，还是作者原本就想回归这些偏移量？&lt;/li&gt;
&lt;li&gt;在Multi Lelve判断时候，直接使用预测的点来判断，是不是没有和GroundTruth联系起来啊？&lt;/li&gt;
&lt;li&gt;centerness是如何训练的？label是否是将原图缩小stride倍，然后根据公式计算得到centerness的真值？&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：FCOS: Fully Convolutional One-Stage Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.01355&quot; target=&quot;_blank&quot; rel=&quot;external&quot;
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>FoveaBox论文笔记</title>
    <link href="http://chr10003566.github.io./2020/02/26/FoveaBox%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/02/26/FoveaBox论文笔记/</id>
    <published>2020-02-26T08:45:16.000Z</published>
    <updated>2020-02-26T13:04:01.610Z</updated>
    
    <content type="html">&lt;p&gt;论文：FoveaBox: Beyond Anchor-based Object Detector&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.03797&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1904.03797&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/taokong/FoveaBox&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/taokong/FoveaBox&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;FoveaBox同样是一篇Anchor Free的文章，与之前介绍的Anchor Free文章不同，FoveaBox并不是通过关键点检测来实现，其思想有点类似DenseBox与YOLOv1，Anchor-based的思想是通过预定义一些Anchor（候选框）去与Groundtruth做比较，得到候选框相对GroundTruth的偏移量，而FoveaBox则将候选框移除了，得到的Feature map对每个pixel预测其分类与候选框的坐标，几乎就是暴力回归。与其他的Anchor free的文章一样，该文章指出Anchor based文章有以下三个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;会带来不少的超参数（anchor的个数、比例等等）&lt;/li&gt;
&lt;li&gt;对于不同的数据集，需要设计不同的anchor的尺寸和比例，泛化能力较差（类似YOLOv3通过聚类得到anchor的大小）&lt;/li&gt;
&lt;li&gt;anchor机制会导致正负样本不平衡的问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述的问题，也确实是Anchor机制导致的问题，相信大家也听过不少了，解决方法要么是提出能够自适应的Anchor，要么放弃Anchor，投入Anchor-free的怀抱。&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5652a56127cc0713d5d532.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Method&quot;&gt;&lt;a href=&quot;#Method&quot; class=&quot;headerlink&quot; title=&quot;Method&quot;&gt;&lt;/a&gt;Method&lt;/h2&gt;&lt;h3 id=&quot;Architecture&quot;&gt;&lt;a href=&quot;#Architecture&quot; class=&quot;headerlink&quot; title=&quot;Architecture&quot;&gt;&lt;/a&gt;Architecture&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e5652fa6127cc0713d5e265.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;首先我们来看看FoveaBox的整体框架是怎样的（见上图），是不是很熟悉，这不是和RetianNet一致的框架么。不同的地方就在与（原文作者也标红了），最后一层的feature map的通道数不相同，RetinaNet是W x H x KA和 W x H x 4A，而FoevaBox则是W x H x K和W x H x 4，其中K表示数据集中物体的类别数（COCO - 80， VOC - 20）而A表示 Anchor的数目，FoevaBox摒弃了Anchor，因此channel数自然少了A倍。&lt;/p&gt;
&lt;h3 id=&quot;Scale-Assignment&quot;&gt;&lt;a href=&quot;#Scale-Assignment&quot; class=&quot;headerlink&quot; title=&quot;Scale Assignment&quot;&gt;&lt;/a&gt;Scale Assignment&lt;/h3&gt;&lt;p&gt;由于不同的物体，尺度变化较大，作者借鉴了FPN的思路，FoveaBox将连续尺度划分成多个区间，并将它们和特征金字塔中的不同层对应。因此，特征金字塔中每层只负责预测某个特定尺度范围的矩形框，其中Sl表示第l层特征的基础大小，而给定的区间范围由一个系数η控制，衡量方式如下二式所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191007201656622.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191007201716471.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中值得注意的是某个尺寸的物体可能会被映射到金字塔不同层次，这与之前FPN的概念有差异（某尺寸只被映射到金字塔特定某层）。&lt;/p&gt;
&lt;h3 id=&quot;Object-Fovea&quot;&gt;&lt;a href=&quot;#Object-Fovea&quot; class=&quot;headerlink&quot; title=&quot;Object Fovea&quot;&gt;&lt;/a&gt;Object Fovea&lt;/h3&gt;&lt;p&gt;Fovea其实就是依据ground truth 在输出的特征上制定的样本区域。给定一个ground truth的坐标（X1,Y1,X2,Y2）,首先将这个矩形框映射到其对应的金字塔层&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191007201745787.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到就是将image上的矩形框的坐标点映射倒FPN输出的每个Feature map上。得到GT映射到Feature map的中心点坐标后，作者通过以下的变换确定中心区域。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2019100720175945.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FoveaBox需要对特征图上每个点都预测一个分类结果以及并回归边框的偏移量，由于物体真实边框附近的点远离物体中心，或者与背景像素更为接近，如果将其作为正样本，对模型的训练造成困难。因此作者通过一个简单的变换，先通过groundtruh边框得到物体中心在特征图中对应的位置，然后通过一个参数σ1调节物体高和宽，将用来训练的真实边框位置向物体中心收缩一点，收缩后的边框内部的点作为正样本，然后使用参数σ2 再次调节边框高和宽，使其向外扩展一点，使用扩展边框外部的点作为负样本(原文是除了正区域外的，被assign的整个feature map区域，作为负样本)，这样将两个边框范围内的点忽略掉，增加正负样本之间的判别度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;σ1 是缩放因子，处于正区域范围内的单元会在训练的时候被标上相应类别标签。而另外一个缩放因子 σ2会被用在制定负区域，同样的计算方式（实验中采用的是σ1=0.3,σ2=0.4）。与此同时，需要注意此时的正样本只占了一小部分，为了克服正负样本不均衡，在分类任务中采用了Focal loss。&lt;/p&gt;
&lt;h3 id=&quot;Box-Prediction&quot;&gt;&lt;a href=&quot;#Box-Prediction&quot; class=&quot;headerlink&quot; title=&quot;Box Prediction&quot;&gt;&lt;/a&gt;Box Prediction&lt;/h3&gt;&lt;p&gt;关于Box 的回归，作者是通过下面那个分支，每个点会预测出(tx1,ty1,tx2,ty2)。其中(x1,y1,x2,y2)表示ground truth坐标，(x,y)表示一个cell单元的坐标，z为标准化因子使得输出空间映射到以1为中心的新空间&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191007201848902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMzc1NjA5,size_16,color_FFFFFF,t_70&quot; alt=&quot;&quot;&gt;&lt;br&gt;在本篇文章中，采用的是Smooth L1 loss&lt;/p&gt;
&lt;p&gt;学习资料：&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/qq_41375609/article/details/102325395&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FoevaBox优质博客&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：FoveaBox: Beyond Anchor-based Object Detector&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.03797&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https:
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>RepPoints论文笔记</title>
    <link href="http://chr10003566.github.io./2020/02/25/RepPoints%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/02/25/RepPoints论文笔记/</id>
    <published>2020-02-25T08:17:05.000Z</published>
    <updated>2020-02-28T12:09:59.010Z</updated>
    
    <content type="html">&lt;p&gt;论文：RepPoints: Point Set Representation for Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/1904.11490.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1904.11490.pdf&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/microsoft/RepPoints&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/microsoft/RepPoints&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h2&gt;&lt;p&gt; 该篇文章是由微软亚研院提出，可以看做是可形变卷积的第三版本，文章相比之前的文章更难理解一些，本人理解可能也有偏差，大家可以多搜索些资料，积极思考下，本文仅给出个人阅读的见解。下面是一些阅读资料。&lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://www.zhihu.com/question/57493889&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;如何评价Deformable Conv&lt;/a&gt;&lt;br&gt; &lt;a href=&quot;https://www.zhihu.com/question/322372759/answer/798327725&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;如何评价RepPoints&lt;/a&gt;&lt;br&gt; 以及本文主要借鉴的一篇博客：&lt;br&gt; &lt;a href=&quot;https://zhuanlan.zhihu.com/p/64522910&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;陀飞轮—RepPoints：可形变卷积的进阶&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Review-of-DCNv1-and-DCNv2&quot;&gt;&lt;a href=&quot;#Review-of-DCNv1-and-DCNv2&quot; class=&quot;headerlink&quot; title=&quot;Review of DCNv1 and DCNv2&quot;&gt;&lt;/a&gt;Review of DCNv1 and DCNv2&lt;/h2&gt;&lt;p&gt;由尺度、姿态、视角和部分形变等因素引起的几何变化是目标识别和检测的主要挑战。在卷积/RoI池化模块中，DCN通过学习采样点的位置来得到几何形变建模的能力。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-1436b85dcf4ff78eb1afe60c27b8820c_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Deformable-Convolution&quot;&gt;&lt;a href=&quot;#Deformable-Convolution&quot; class=&quot;headerlink&quot; title=&quot;Deformable Convolution&quot;&gt;&lt;/a&gt;Deformable Convolution&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-6eacb5f7ab764e0a13c26ebb2414eba1_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;由输入特征学习得到的偏移量来改变标准卷积的采样位置。&lt;/p&gt;
&lt;p&gt;可形变卷积可以表示为：&lt;br&gt;&lt;img src=&quot;https://www.zhihu.com/equation?tex=y%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D+w_%7Bk%7D+%5Ccdot+x%5Cleft%28p%2Bp_%7Bk%7D%2B%5CDelta+p_%7Bk%7D%5Cright%29%5C%5C&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;[具体的解释可见上述链接]&lt;/p&gt;
&lt;p&gt;PS：偏移量特征的分辨率与输入特征的分辨率相同，且通道数为采样点个数的两倍(即每个位置都有x和y两个方向的偏移量)。&lt;/p&gt;
&lt;h3 id=&quot;Modulated-Deformable-Convolution&quot;&gt;&lt;a href=&quot;#Modulated-Deformable-Convolution&quot; class=&quot;headerlink&quot; title=&quot;Modulated Deformable Convolution&quot;&gt;&lt;/a&gt;Modulated Deformable Convolution&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-5763a8a1b84c0a9ea1251847eeaffd62_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;相比可形变卷积多了一个调制因子。&lt;/p&gt;
&lt;p&gt;调制可形变卷积可以表示为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.zhihu.com/equation?tex=y%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D+w_%7Bk%7D+%5Ccdot+x%5Cleft%28p%2Bp_%7Bk%7D%2B%5CDelta+p_%7Bk%7D%5Cright%29+%5Ccdot+%5CDelta+m_%7Bk%7D%5C%5C&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;PS：调制因子特征的分辨率与输入特征的分辨率相同，且通道数为采样点的个数，加上偏移量特征后的通道数为采样点个数的三倍(即每个位置都有x和y两个方向的偏移量，还有一个调制因子)。&lt;/p&gt;
&lt;h3 id=&quot;Deformable-RoI-Pooling&quot;&gt;&lt;a href=&quot;#Deformable-RoI-Pooling&quot; class=&quot;headerlink&quot; title=&quot;Deformable RoI Pooling&quot;&gt;&lt;/a&gt;Deformable RoI Pooling&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-a0f767e67fbabf461d2abd479ba90da8_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;RepPoints&quot;&gt;&lt;a href=&quot;#RepPoints&quot; class=&quot;headerlink&quot; title=&quot;RepPoints&quot;&gt;&lt;/a&gt;RepPoints&lt;/h2&gt;&lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;p&gt;在目标检测任务中，边界框描述了目标检测器各阶段的目标位置。&lt;/p&gt;
&lt;p&gt;虽然边界框便于计算，但它们仅提供目标的粗略定位，并不完全拟合目标的形状和姿态。因此，从边界框的规则单元格中提取的特征可能会受到背景内容或前景区域的无效信息的严重影响。这可能导致特征质量降低，从而降低了目标检测的分类性能。&lt;/p&gt;
&lt;p&gt;本文提出一种新的表示方法，称为 RepPoints，它提供了更细粒度的定位和更方便的分类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-90b287d33671ba1544fa99950f6b9058_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，RepPoints 是一组点，通过学习自适应地将自己置于目标之上，该方式限定了目标的空间范围，并且表示具有重要语义信息的局部区域。&lt;/p&gt;
&lt;p&gt;RepPoints 的训练由目标定位和识别共同驱动的，因此，RepPoints 与 ground-truth 的边界框紧密相关，并引导检测器正确地分类目标。&lt;/p&gt;
&lt;h3 id=&quot;Bounding-Box-Representation&quot;&gt;&lt;a href=&quot;#Bounding-Box-Representation&quot; class=&quot;headerlink&quot; title=&quot;Bounding Box Representation&quot;&gt;&lt;/a&gt;Bounding Box Representation&lt;/h3&gt;&lt;p&gt;边界框是一个4维表示，编码目标的空间位置，即&lt;img src=&quot;https://www.zhihu.com/equation?tex=%5Cmathcal%7BB%7D%3D%28x%2C+y%2C+w%2C+h%29&quot; alt=&quot;&quot;&gt; ， &lt;img src=&quot;https://www.zhihu.com/equation?tex=x%2C+y&quot; alt=&quot;&quot;&gt;表示中心点， &lt;img src=&quot;https://www.zhihu.com/equation?tex=w%2C+h&quot; alt=&quot;&quot;&gt;表示宽度和高度。&lt;/p&gt;
&lt;p&gt;由于其使用简单方便，现代目标检测器严重依赖于边界框来表示检测 pipeline 中各个阶段的对象。&lt;/p&gt;
&lt;p&gt;性能最优的目标检测器通常遵循一个 multi-stage 的识别范式，其中目标定位是逐步细化的。其中，目标表示的角色如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-e7d1addcb3d0c80813d18c0455972ad3_1440w.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：RepPoints: Point Set Representation for Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/1904.11490.pdf&quot; target=&quot;_blank&quot; rel=&quot;e
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>ExtremeNet论文笔记</title>
    <link href="http://chr10003566.github.io./2020/02/20/ExtremeNet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/02/20/ExtremeNet论文笔记/</id>
    <published>2020-02-20T04:03:42.000Z</published>
    <updated>2020-02-20T08:11:27.481Z</updated>
    
    <content type="html">&lt;p&gt;论文：Bottom-up Object Detection by Grouping Extreme and Center Points&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1901.08043&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1901.08043&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/xingyizhou/ExtremeNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/xingyizhou/ExtremeNet&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h1&gt;&lt;p&gt;本篇文章与上一篇文章Objects as points是同一个第一作者，查阅了下作者的主页&lt;a href=&quot;https://www.cs.utexas.edu/~zhouxy/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Xingyi Zhou主页&lt;/a&gt;，发现作者主要是做关键点检测，正巧将其使用在目标检测上。本篇文章被CVPR 2019收录了，同样是Anchor free系列中的文章，也同样是关键点检测转到目标检测当中！&lt;/p&gt;
&lt;p&gt;#Introduction&lt;br&gt;文章作者提到自上而下的检测器—基于Anchor based的检测器存在以下问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矩形框不是自然对象的表示，大多数的物体不是轴对齐的框&lt;/li&gt;
&lt;li&gt;基于Anchor based的检测器需要枚举大量的矩形框，而这些候选框并不是真正理解了物体本身构成的视觉语法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文还与CornerNet做了比较，主要有两点不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;角点，往往在物体的外面，没有明显的外观特点。而极点，则位于物体上，有一致的局部外观特征。&lt;/li&gt;
&lt;li&gt;CornerNet是基于几何重组，即通过计算角点的嵌入向量的距离，来判断两个角点是否属于同一个矩形框，而ExtremeNet则是通过四个极点+一个中心点来构建矩形框的，四个极点和中心点都包含外观特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Method&quot;&gt;&lt;a href=&quot;#Method&quot; class=&quot;headerlink&quot; title=&quot;Method&quot;&gt;&lt;/a&gt;Method&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4e297148b86553eed44aa6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;上图是ExtremeNet的整体框架图，可以看到输入一张图像后，通过沙漏网络最后会得到4个C x H x W的Heatmap（极上点，极左点，极下点，极右点），以及1个C x h x w的Heatmap（中心点），还有4个极点的偏移值（2 x H x W）。如何通过4个极点确定一个候选框呢？作者借鉴了&lt;a href=&quot;https://arxiv.org/abs/1708.02750&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Extreme clicking for efficient object annotation&lt;/a&gt;，即若4个极点的坐标为(x(t), y(t)), (x(l), y(l)), (x(b), y(b)), (x(r), y(r))，则中心点的坐标为(x(l)+x(r)/2,y(t)+y(b)/2)，如何通过Heatmap确定极点的呢？作者通过一个3 x 3的滑动窗口，找到一个Peak，它的值比相邻的关键点都大，并且比设定的阈值也更大。因此确定了极点。4个极点的分类损失和偏移值的使用均和CornerNet相一致，中心点的Heatmap目的则是为了减少误检，用于四个极点固定好后，可以计算得到候选框的中心点坐标（下采样后的），如果计算得到的中心点在中心点的Heatmap的值大于设定的阈值，则保留该候选框。下图则展示了其具体的流程～&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4e347e48b86553eed69439.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者针对一些特殊情况，做了一些改进，如当三个objects并排的时候，可能会出现最左边的极点和最右边的极点与最上、最下的极点同样构成一个候选框，也就是如下图的情况。&lt;br&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-6ba5861194a662374d321158df519ba1_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;蓝色代表真实目标，红色虚线代表被误检的框。如果出现这种情况，即一个框内所有框的置信度分数之和超过该框的3倍，则将该框的置信度除以2&lt;/p&gt;
&lt;p&gt;由于极点的定义不是固定的，如果目标物边界边缘构成极点，则该边的任意一个点都可以看作是极点。因此，本文对目标物的对齐边界产生一个较弱的相应，而不是强峰值响应。但这种弱相应存在两个问题：第一：较弱的相应其值可能会低于峰值阈值，因此该极点可能会被忽略。第二：即使检测到了一个极点，其分数仍可能会比具有强响应的旋转对象的分数低。本文采用edge aggregation解决上述问题。&lt;/p&gt;
&lt;p&gt;对于提取出局部最大点的极点，将其水平方向及垂直方向极点的分数进行聚合。将所有分数单调递减的极点进行聚合。当在聚合方向达到局部最小值的时候停止聚合。&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4e3e8e48b86553eed8b0e3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Experiment&quot;&gt;&lt;a href=&quot;#Experiment&quot; class=&quot;headerlink&quot; title=&quot;Experiment&quot;&gt;&lt;/a&gt;Experiment&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4e3ed448b86553eed8bd67.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;并未给出速度上的实验，不过从实验效果看，可以看到Center heatmap和group确实可以在CornerNet上得到提升。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：Bottom-up Object Detection by Grouping Extreme and Center Points&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1901.08043&quot; target=&quot;_blank&quot; re
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>CenterNet论文笔记-Objects as points</title>
    <link href="http://chr10003566.github.io./2020/02/19/CenterNet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Objects-as-points/"/>
    <id>http://chr10003566.github.io./2020/02/19/CenterNet论文笔记-Objects-as-points/</id>
    <published>2020-02-19T07:21:19.000Z</published>
    <updated>2020-02-19T11:28:20.198Z</updated>
    
    <content type="html">&lt;p&gt;论文：Objects as points&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.07850&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1904.07850&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/xingyizhou/CenterNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/xingyizhou/CenterNet&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h1&gt;&lt;p&gt;这篇文章（Objects as points）同样构建的模型同样也叫CenterNet，一个研究团队是来自德克萨斯州大学的学生，另一篇则是来自国科大和华为诺亚实验室。两篇共同点都是基于CornerNet的框架，对其做了一定程度的改进，但改进方向不相同，本篇CenterNet主要是借鉴了CornerNet从关键点检转到目标检测的方法，而华为诺亚实验室的CenterNet则主要是找到了CornerNet存在未利用到物体内部特征，无法感知内部信息的缺点，对该缺点做了改进。简单介绍了两篇CenterNet，现在着重介绍下本篇CenterNet吧。&lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;在本文中，作者很专业地指出了one stage approach和two stage approach的区别&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One stage approach: 在图像上滑动复杂排列的可能bbox（即锚点）,然后直接对框进行分类，而不会利用到物体内部特征，无法感知内部信息。&lt;/p&gt;
&lt;p&gt;Two stage approach:对每个潜在框会重新计算图像特征，然后将那些特征进行分类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;并且作者认为后处理，即 NMS（非极大值抑制），通过计算Bbox间的IOU来删除同个目标的重复检测框。这种后处理很难区分和训练，因此现有大多检测器都不是端到端可训练的。&lt;/p&gt;
&lt;p&gt;作者将自己的方法CenterNet与one stage approach做了详细的对比，认为主要存在以下三点区别&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;由于本文的CenterNet没有设置Anchor，而是通过关键点，因此不用设置阈值来区分前后景&lt;/p&gt;
&lt;p&gt;同样也是由于Anchor Free的原因，不需要使用NMS。&lt;/p&gt;
&lt;p&gt;CenterNet与之前目标检测框架比起来，使用了更大的特征图，仅仅只缩放了4倍，而此前的目标检测框架往往缩放8，16，32倍等。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4d131448b86553ee9df3e3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到算法在效果和效率之间能取得很好的平衡！&lt;/p&gt;
&lt;h1 id=&quot;Method&quot;&gt;&lt;a href=&quot;#Method&quot; class=&quot;headerlink&quot; title=&quot;Method&quot;&gt;&lt;/a&gt;Method&lt;/h1&gt;&lt;p&gt;这里主要和CornerNet做一个对比，来详细的了解一下CenterNet所做的创新（个人感觉相比于华为诺亚实验室提高5%的创新点，本文稍显简单）此处借鉴：&lt;a href=&quot;本文链接：https://blog.csdn.net/u014380165/article/details/92801206&quot;&gt;AI之路-CenterNet笔记&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CenterNet与CornerNet相同，都是采用&lt;strong&gt;Heatmap来进行预测&lt;/strong&gt;，所不同的是CenterNet预测的是中心点，而CornerNet则预测的是左上角点和右下角点，CenterNet在关键点的处理上是借鉴CornerNet的，&lt;strong&gt;引入了预测点的高斯分布区域计算真实预测值&lt;/strong&gt;，对该点不是很理解的同学，可以阅读&lt;a href=&quot;https://zhuanlan.zhihu.com/p/66048276&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CenterNet——Objects as Points论文解读&lt;/a&gt;，引用一张图来简单说明一下。&lt;img src=&quot;https://pic2.zhimg.com/80/v2-82171546bc2b8448d45dd6915c40d375_hd.png&quot; alt=&quot;高斯分布&quot;&gt;即真实object中心点处Heatmap的值为1，而靠近中心点的值不完全设置为0，而是呈高斯分布逐渐减小。在损失函数的设计上，CenterNet同样借鉴了CornerNet，将Focal loss融合进自己中心点检测的损失函数，在上面的链接中非常详细的解释了该损失函数的原理。&lt;/li&gt;
&lt;li&gt;CenterNet和CornerNet同样使用了offset来对下采样导致的坐标误差进行微调，但是CornerNet使用的是在坐标误差中多次被应用的Smooth L1 loss，而CenterNet则使用的是L1 loss。其实这个offset是可选的，并非一定要使用，只是使用的话效果会有所下降（做比赛，刷榜的时候可以考虑的策略）&lt;/li&gt;
&lt;li&gt;CornerNet是通过组合左上角点和右上角点来组成候选框的，那么CenterNet如何通过中心点来获得候选框呢？CenterNet则是使用暴力法，直接通过回归得到候选框的宽和高，同样宽、高的损失函数同样是L1 loss，因此其整体的损失函数由3部分组成，第一部分是中心点分类的损失，第二部分是尺寸（宽、高）的损失，第三部分则是中心点偏移点的损失&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4d168148b86553ee9f10b0.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此可以推测出（论文竟然没给框架图），CenterNet预测得到三种Heatmap，第一种中心点分类：[1,C（类别数）,128,128]、[1,2（宽、高）,128,128]、[1,2（偏移量）,128,128]。&lt;br&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4d17ad48b86553ee9f79d4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Experiment&quot;&gt;&lt;a href=&quot;#Experiment&quot; class=&quot;headerlink&quot; title=&quot;Experiment&quot;&gt;&lt;/a&gt;Experiment&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://pic.downk.cc/item/5e4d1b0148b86553eea08cb6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到与大部分检测相比，CenterNet的FPS几乎碾压，准确率也能保持在中上的水平。&lt;/p&gt;
&lt;h1 id=&quot;Note&quot;&gt;&lt;a href=&quot;#Note&quot; class=&quot;headerlink&quot; title=&quot;Note&quot;&gt;&lt;/a&gt;Note&lt;/h1&gt;&lt;p&gt;本文CenterNet在检测的时间和效果上做到了比较好的权衡，虽然算法也相对简单，但能将同一个框架应用到不同的领域，是十分棒的一个模型！&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：Objects as points&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.07850&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1904.07850&lt;/
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>CenterNet论文笔记.md</title>
    <link href="http://chr10003566.github.io./2020/02/16/CenterNet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://chr10003566.github.io./2020/02/16/CenterNet论文笔记/</id>
    <published>2020-02-16T04:14:49.000Z</published>
    <updated>2020-02-16T08:31:01.824Z</updated>
    
    <content type="html">&lt;p&gt;论文：CenterNet: Keypoint Triplets for Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.08189&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1904.08189&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/Duankaiwen/CenterNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/Duankaiwen/CenterNet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;今天介绍一篇很好的 one-stage 目标检测论文：CenterNet: Keypoint Triplets for Object Detection，该论文是由 中科院，牛津大学以及华为诺亚方舟实验室联合提出。截至目前，据我们所知，CenterNet 应该是 one-stage 目标检测方法中性能最好的方法。&lt;/p&gt;
&lt;p&gt;《CenterNet: Keypoint Triplets for Object Detection》&lt;/p&gt;
&lt;p&gt;作者团队：中科院，牛津大学，华为诺亚方舟实验室&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;基于关键点的目标检测方法例如最具代表性的CornerNet，通过检测物体的左上角点和右下角点来确定目标，但在确定目标的过程中，无法有效利用物体的内部的特征，即无法感知物体内部的信息，从而导致该类方法产生了很多误检 (错误目标框)。本文利用关键点三元组即中心点、左上角点和右下角点三个关键点而不是两个点来确定一个目标，使网络花费了很小的代价便具备了感知物体内部信息的能力，从而能有效抑制误检。另外，为了更好的检测中心点和角点，我们分别提出了 center pooling 和 cascade corner pooling 来提取中心点和角点的特征。我们方法的名字叫 CenterNet，是一种 one-stage 的方法，在最具挑战性之一的数据集 MS COCO [2] 上，获得了47% AP，超过了所有已知的 one-stage 检测方 法，并大幅度领先，其领先幅度至少达 4.9%。&lt;/p&gt;
&lt;h2 id=&quot;Basis&quot;&gt;&lt;a href=&quot;#Basis&quot; class=&quot;headerlink&quot; title=&quot;Basis&quot;&gt;&lt;/a&gt;Basis&lt;/h2&gt;&lt;p&gt;我们抑制误检的原理基于以下推论：如果目标框是准确的，那么在其中心区域能够检测到目标中心点的概率就会很高，反之亦然。因此，首先利用左上和右下两个角点生成初始目标框，对每个预测框定义一个中心区域，然后判断每个目标框的中心区域是否含有中心点，若有则保留该目标框，若无则删除该目标框，其原理如图1所。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-8a8af5135e730d0b77f9303fc9c25a98_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Baseline-and-Motivation&quot;&gt;&lt;a href=&quot;#Baseline-and-Motivation&quot; class=&quot;headerlink&quot; title=&quot;Baseline and Motivation&quot;&gt;&lt;/a&gt;Baseline and Motivation&lt;/h2&gt;&lt;p&gt;其实不光是基于关键点的 one-stage 方法无法感知物体内部信息，几乎所有的 one-stage 方法都存在这一问题。本论文的 baseline 为 CornerNet，因此首先讨论 CornerNet 为什么容易产生很多的误检。首先，CornerNet 通过检测角点确定目标，而不是通过初始候选框 anchor 的回归确定目标，由于没有了 anchor 的限制，使得任意两个角点都可以组成一个目标框，这就对判断两个角点是否属于同一物体的算法要求很高，一但准确度差一点，就会产生很多错误目标框。其次，恰恰这个算法有缺陷。因为此算法在判断两个角点是否属于同一物体时，缺乏全局信息的辅助，因此很容易把原本不是同一物体的两个角点看成是一对角点，因此产生了很多错误目标框。最后，角点的特征对边缘比较敏感，这导致很多角点同样对背景的边缘很敏感，因此在背景处也检测到了错误的角点。综上原因，使得 CornerNet 产生了很多误检。如图2所示，我们用 CornerNet 对两张图片进行检测，根据每个预测目标框的 confidence 选出 top100 个预测框 (根据 MS COCO 标准)，左图只显示了中等尺度和大尺度的预测框，右图只显示了小尺度的预测框，可以发现产生了很多误检。其中蓝色框为 ground truth, 红色框为预测框。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-da0900ab5cda899f71efaf3ce605d6c0_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;其实不光 CornerNet 有这个问题，实际上 anchor-based one-stage detector 也存在这个问题，因为此类方法直接对 anchor 进行回归和分类，这个过程并没有像 two-stage 方法一样利用到了物体内部特征，因此无法感知物体内部信息，就会和 CornerNet 一样产生很多误检。下面是我用 SSD512 检测了两张图片，显示了 top100 个检测框，出现了和 CornerNet 类似的问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-c6529f46061695068e4c05bcc7411de9_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;为了能够量化的分析误检问题，我们提出了一种新的衡量指标，称为FD (false discovery) rate, 此指标能够很直观的反映出误检情况。FD rate 的计算方式为 FD = 1-AP， 其中 AP 为 IoU 阈值取[0.05 : 0.05 : 0.5]下的平均精度。我们统计了 CornerNet 的误检情况，如表1所示：&lt;br&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-e6a1b19435c160e48a298e6d9bd7bf63_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到，FD = 37.8，而 [公式] 高达32.7，这意味着即使我们把条件限制的很严格：只有那些与 ground-truth 的 IoU&amp;lt; 0.05 的才被认定为错误目标框，每100个预测框中仍然平均有32.7 个错误目标框！而小尺度的目标框其FD更是达到了60.3！&lt;/p&gt;
&lt;p&gt;我们分析出了 CornerNet 的问题后，接下来就是找出解决之道，关键问题在于让网络具备感知物体内部信息的能力。一个较容易想到的方法是把 CornerNet 变成一个 two-stage 的方法，即利用 RoI pooling 或 RoI align 提取预测框的内部信息，从而获得感知能力。但这样做开销很大，因此我们提出了用关键点三元组来检测目标，这样使得我们的方法在 one-stage 的前提下就能获得感知物体内部信息的能力。并且开销较小，因为我们只需关注物体的中心，从而避免了 RoI pooling 或 RoI align 关注物体内部的全部信息。&lt;/p&gt;
&lt;h2 id=&quot;Method&quot;&gt;&lt;a href=&quot;#Method&quot; class=&quot;headerlink&quot; title=&quot;Method&quot;&gt;&lt;/a&gt;Method&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Structure &lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-bbea0d67bb5dbcdf6fd16141ed229071_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上图为 CenterNet 的结构图。网络通过 center pooling 和 cascade corner pooling 分别得到 center heatmap 和 corner heatmaps，用来预测关键点的位置。得到角点的位置和类别后，通过 offsets 将角点的位置映射到输入图片的对应位置，然后通过 embedings 判断哪两个角点属于同一个物体，以便组成一个检测框。正如前文所说，组合过程中由于缺乏来自目标区域内部信息的辅助，从而导致大量的误检。为了解决这一问题，CenterNet 不仅预测角点，还预测中心点。我们对每个预测框定义一个中心区域，通过判断每个目标框的中心区域是否含有中心点，若有则保留，并且此时框的 confidence 为中心点，左上角点和右下角点的confidence的平均，若无则去除，使得网络具备感知目标区域内部信息的能力，能够有效除错误的目标框。&lt;/p&gt;
&lt;p&gt;我们发现中心区域的尺度会影响错误框去除效果。中心区域过小导致很多准确的小尺度的目标也会被去除，而中心区域过大导致很多大尺度的错误目标框无法被去除，因此我们提出了尺度可调节的中心区域定义法 (公式1)。该方法可以在预测框的尺度较大时定义一个相对较小的中心区域，在预测框的尺度较小时预测一个相对较大的中心区域。如 Fig3 所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-13133a7cea3b9330f170d66bc1713a88_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-a0418bc3fc012942b1b3bc533effba40_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Center pooling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Center pooling：一个物体的中心并不一定含有很强的，易于区分于其他类别的语义信息。例如，一个人的头部含有很强的，易于区分于其他类别的语义信息，但是其中心往往位于人的中部。我们提出了center pooling 来丰富中心点特征。图5为该方法原理，center pooling提取中心点水平方向和垂直方向的最大值并相加，以此给中心点提供所处位置以外的信息。这一操作使中心点有机会获得更易于区分于其他类别的语义信息。Center pooling 可通过不同方向上的 corner pooling 的组合实现。一个水平方向上的取最大值操作可由 left pooling 和 right pooling通过串联实现，同理，一个垂直方向上的取最大值操作可由 top pooling 和 bottom pooling通过串联实现，如图6所示。&lt;br&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-70b5a7db0ad2cfd1b668c671c94c6d9c_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-6cb703e827176d3aefbfff599b54e09a_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cascade corner pooling:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般情况下角点位于物体外部，所处位置并不含有关联物体的语义信息，这为角点的检测带来了困难。图7(a) 为传统做法，称为 corner pooling。它提取物体边界最大值并相加，该方法只能提供关联物体边缘语义信息，对于更加丰富的物体内部语义信息则很难提取到。图7(b)为cascade corner pooling 原理，它首先提取物体边界最大值，然后在边界最大值处继续向内部(图中沿虚线方向)提取提最大值，并与边界最大值相加，以此给角点特征提供更加丰富的关联物体语义信息。Cascade corner pooling 也可通过不同方向上的 corner pooling 的组合实现，如图8 所示，图8展示了cascade left corner pooling 原理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-383c1b465125216eb6f6fe9fd6457d61_hd.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/62789701&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CenterNet&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;论文：CenterNet: Keypoint Triplets for Object Detection&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1904.08189&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ht
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>mmDetection源码分析（五）：Faster R-CNN模块解读（三）</title>
    <link href="http://chr10003566.github.io./2019/12/11/mmdetection(5)/"/>
    <id>http://chr10003566.github.io./2019/12/11/mmdetection(5)/</id>
    <published>2019-12-11T11:04:53.000Z</published>
    <updated>2019-12-11T11:06:02.489Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
      <category term="mmDetection源码分析" scheme="http://chr10003566.github.io./categories/mmDetection%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>mmDetection源码分析（四）：Faster R-CNN模块解读（二）</title>
    <link href="http://chr10003566.github.io./2019/12/10/mmdetection(4)/"/>
    <id>http://chr10003566.github.io./2019/12/10/mmdetection(4)/</id>
    <published>2019-12-10T13:51:03.000Z</published>
    <updated>2019-12-11T10:58:30.665Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Faster-R-CNN模块解读（二）-Backbone的构建&quot;&gt;&lt;a href=&quot;#Faster-R-CNN模块解读（二）-Backbone的构建&quot; class=&quot;headerlink&quot; title=&quot;Faster R-CNN模块解读（二） Backbone的构建&quot;&gt;&lt;/a&gt;Faster R-CNN模块解读（二） Backbone的构建&lt;/h1&gt;&lt;p&gt;上一篇博客介绍了检测器的构建，是通过&lt;code&gt;build_from_cfg(cfg.model, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))&lt;/code&gt;调用得到，而构建检测器时，必须要构建Backbone，在&lt;code&gt;TwoStageDetector&lt;/code&gt;类初始化时，就需要构建&lt;code&gt;self.backbone = builder.build_backbone(backbone)&lt;/code&gt;。而在&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/builder.py&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cfg, registry, default_args=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(cfg, list):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        modules = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            build_from_cfg(cfg_, registry, default_args) &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; cfg_ &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; nn.Sequential(*modules)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; build_from_cfg(cfg, registry, default_args)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_backbone&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cfg)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; build(cfg, BACKBONES)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到同样是调用&lt;code&gt;build_from_cfg(cfg.model.backbone, BACKBONES, dict(train_cfg=train_cfg, test_cfg=test_cfg))&lt;/code&gt;来构建Backbone的。&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;&lt;em&gt;mmdetection/configs/faster_rcnn_r50_fpn_1x.py&lt;/em&gt;&lt;/strong&gt;中，backbone的字典参数如下：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;backbone=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;ResNet&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        depth=&lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_stages=&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_indices=(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        frozen_stages=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        style=&lt;span class=&quot;string&quot;&gt;&#39;pytorch&#39;&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;此处&lt;code&gt;type&lt;/code&gt;为ResNet，则将会去构造&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/backbones/resnet.py&lt;/em&gt;&lt;/strong&gt;这个类。&lt;/p&gt;
&lt;p&gt;该类则是大名鼎鼎的ResNet，下面将会与&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ResNet论文&lt;/a&gt; 中的网络结构一起来学习ResNet的实现。主要介绍ResNet-18与ResNet-50。&lt;br&gt;&lt;img src=&quot;http://www.programmersought.com/images/768/3133597021d9210d2d924d3d9ec5d388.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;对网络结构不是很熟悉的同学，可能还要借助caffe的prototxt可视化一起，会有更深层的理解，也会对编写该代码的作者更加崇拜。&lt;/p&gt;
&lt;p&gt;ResNet-18 prototxt：&lt;a href=&quot;https://raw.githubusercontent.com/marvis/pytorch-caffe-darknet-convert/master/cfg/resnet-18.prototxt&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://raw.githubusercontent.com/marvis/pytorch-caffe-darknet-convert/master/cfg/resnet-18.prototxt&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;可视化网页：&lt;br&gt;&lt;a href=&quot;https://ethereon.github.io/netscope/#/editor&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://ethereon.github.io/netscope/#/editor&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;将prototxt复制进左边的输入框，shift+enter即可以看到可视化的网络结构。&lt;/p&gt;
&lt;p&gt;ResNet-50，何恺明大大已经可视化了：&lt;a href=&quot;http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;准备工作做好后，我们便正式开始学习Backbone（ResNet）的构建了。&lt;/p&gt;
&lt;p&gt;先跳过&lt;code&gt;BasicBlock&lt;/code&gt;，&lt;code&gt;Bottleneck&lt;/code&gt;这两个类，看&lt;code&gt;ResNet&lt;/code&gt;这个类。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;__init__&lt;/code&gt;函数之前，&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;arch_settings = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;number&quot;&gt;18&lt;/span&gt;: (BasicBlock, (&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;number&quot;&gt;34&lt;/span&gt;: (BasicBlock, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;: (Bottleneck, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;number&quot;&gt;101&lt;/span&gt;: (Bottleneck, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;23&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;number&quot;&gt;152&lt;/span&gt;: (Bottleneck, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;36&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;代码中很清晰的将不同层数的ResNet的区别，以及不同层数的ResNet的配置都一一告知。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             depth,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             num_stages=&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             strides=&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             dilations=&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             out_indices=&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             style=&lt;span class=&quot;string&quot;&gt;&#39;pytorch&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             frozen_stages=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             conv_cfg=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             norm_cfg=dict&lt;span class=&quot;params&quot;&gt;(type=&lt;span class=&quot;string&quot;&gt;&#39;BN&#39;&lt;/span&gt;, requires_grad=True)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             norm_eval=True,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             dcn=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             stage_with_dcn=&lt;span class=&quot;params&quot;&gt;(False, False, False, False)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             gcb=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             stage_with_gcb=&lt;span class=&quot;params&quot;&gt;(False, False, False, False)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             gen_attention=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             stage_with_gen_attention=&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;, &lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;, &lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;, &lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             with_cp=False,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;             zero_init_residual=True)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    super(ResNet, self).__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; depth &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.arch_settings:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; KeyError(&lt;span class=&quot;string&quot;&gt;&#39;invalid depth &amp;#123;&amp;#125; for resnet&#39;&lt;/span&gt;.format(depth))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.depth = depth&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.num_stages = num_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; num_stages &amp;gt;= &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; num_stages &amp;lt;= &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.strides = strides&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.dilations = dilations&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; len(strides) == len(dilations) == num_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.out_indices = out_indices&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; max(out_indices) &amp;lt; num_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.style = style&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.frozen_stages = frozen_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.conv_cfg = conv_cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.norm_cfg = norm_cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.with_cp = with_cp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.norm_eval = norm_eval&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.dcn = dcn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.stage_with_dcn = stage_with_dcn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; dcn &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; len(stage_with_dcn) == num_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.gen_attention = gen_attention&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.gcb = gcb&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.stage_with_gcb = stage_with_gcb&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; gcb &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; len(stage_with_gcb) == num_stages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.zero_init_residual = zero_init_residual&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.block, stage_blocks = self.arch_settings[depth]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.stage_blocks = stage_blocks[:num_stages]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.inplanes = &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self._make_stem_layer()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.res_layers = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i, num_blocks &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(self.stage_blocks):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        stride = strides[i]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        dilation = dilations[i]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        dcn = self.dcn &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.stage_with_dcn[i] &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        gcb = self.gcb &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.stage_with_gcb[i] &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        planes = &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;**i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        res_layer = make_res_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.block,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            num_blocks,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            stride=stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            dilation=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            style=self.style,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            with_cp=with_cp,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg=conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            norm_cfg=norm_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            dcn=dcn,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            gcb=gcb,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            gen_attention=gen_attention,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            gen_attention_blocks=stage_with_gen_attention[i])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.inplanes = planes * self.block.expansion&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        layer_name = &lt;span class=&quot;string&quot;&gt;&#39;layer&amp;#123;&amp;#125;&#39;&lt;/span&gt;.format(i + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.add_module(layer_name, res_layer)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.res_layers.append(layer_name)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self._freeze_stages()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.feat_dim = self.block.expansion * &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;**(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        len(self.stage_blocks) - &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;从&lt;code&gt;__init__&lt;/code&gt;函数中，我们可以看到，首先是将一系列的卷积层的变量传递到该类当中，比如：&lt;code&gt;stride（步长）&lt;/code&gt;，&lt;code&gt;dilation（扩张率）&lt;/code&gt;等，之后调用&lt;code&gt;_make_stem_layer()&lt;/code&gt;。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;_make_stem_layer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.conv1 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        kernel_size=&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        stride=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        padding=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt;, postfix=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.add_module(self.norm1_name, norm1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.relu = nn.ReLU(inplace=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    self.maxpool = nn.MaxPool2d(kernel_size=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, stride=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, padding=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;从这个类，我们可以看到定义了conv1是一个7x7、步长为2，padding为3，输入channel为3，输出的channel为64，之后再接BatchNorm层与ReLU层，最后接一个kernel_size为3，步长为2的maxPooling 层&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pic2.superbed.cn/item/5df085e11f8f59f4d6db8aaf.jpg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;img src=&quot;https://pic2.superbed.cn/item/5df085e11f8f59f4d6db8aaf&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看到与可视化的第一部分一致，经过conv1与maxPooling之后，Feature map的尺寸变成原来的1/4。&lt;/p&gt;
&lt;p&gt;之后便是一个循环，构造每个stage的卷积层。通过调用&lt;code&gt;make_res_layer&lt;/code&gt;，构造每个stage的卷积层。&lt;code&gt;make_res_layer&lt;/code&gt;有个参数&lt;code&gt;block&lt;/code&gt;，用于指定使用的是&lt;code&gt;BasicBlock&lt;/code&gt;还是&lt;code&gt;Bottleneck&lt;/code&gt;。通过&lt;code&gt;__init__&lt;/code&gt;中的&lt;br&gt;&lt;code&gt;self.block, stage_blocks = self.arch_settings[depth]&lt;/code&gt;设置，可以看到ResNet-18使用的是&lt;code&gt;BasicBlock&lt;/code&gt;，而ResNet-50使用的是&lt;code&gt;Bottleneck&lt;/code&gt;。两者的区别可以从图1不同层数的ResNet结构图看出，BasicBlock是堆叠了两个3x3的卷积核，&lt;code&gt;Bottleneck&lt;/code&gt;则是1x1-&amp;gt;3x3-&amp;gt;1x1。具体结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.programmersought.com/images/562/306cffd755e3f5fa09876452a583b4ca.png&quot; alt=&quot;&quot;&gt; 通过代码，我们同样可以看出区别。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;BasicBlock&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    expansion = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv1 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            stride=stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            padding=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            dilation=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.add_module(self.norm1_name, norm1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv2 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg, planes, planes, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, padding=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.add_module(self.norm2_name, norm2)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.relu = nn.ReLU(inplace=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.downsample = downsample&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.stride = stride&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.dilation = dilation&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; with_cp&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;而&lt;code&gt;Bottleneck&lt;/code&gt;，此处的代码忽略了一些判断，dcn，gcb等。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Bottleneck&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    expansion = &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     self.conv1 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            kernel_size=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            stride=self.conv1_stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.add_module(self.norm1_name, norm1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fallback_on_stride = &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.with_modulated_dcn = &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_dcn:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            fallback_on_stride = dcn.get(&lt;span class=&quot;string&quot;&gt;&#39;fallback_on_stride&#39;&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.with_modulated_dcn = dcn.get(&lt;span class=&quot;string&quot;&gt;&#39;modulated&#39;&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; self.with_dcn &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; fallback_on_stride:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.conv2 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                kernel_size=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                stride=self.conv2_stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                padding=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                dilation=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv3 = build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes * self.expansion,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            kernel_size=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.add_module(self.norm3_name, norm3)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.relu = nn.ReLU(inplace=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.downsample = downsample&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;简单了解了下&lt;code&gt;BasicBlock&lt;/code&gt;和&lt;code&gt;Bottleneck&lt;/code&gt;之后，我们来看下&lt;code&gt;make_res_layer&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;make_res_layer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(block,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   blocks,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   stride=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   dilation=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   style=&lt;span class=&quot;string&quot;&gt;&#39;pytorch&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   with_cp=False,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   conv_cfg=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   norm_cfg=dict&lt;span class=&quot;params&quot;&gt;(type=&lt;span class=&quot;string&quot;&gt;&#39;BN&#39;&lt;/span&gt;)&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   dcn=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   gcb=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   gen_attention=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                   gen_attention_blocks=[])&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    downsample = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; stride != &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; inplanes != planes * block.expansion:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        downsample = nn.Sequential(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            build_conv_layer(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                planes * block.expansion,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                kernel_size=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                stride=stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            build_norm_layer(norm_cfg, planes * block.expansion)[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        )&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    layers = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    layers.append(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        block(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            inplanes=inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            planes=planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            stride=stride,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            dilation=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            downsample=downsample,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            style=style,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            with_cp=with_cp,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            conv_cfg=conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            norm_cfg=norm_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            dcn=dcn,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            gcb=gcb,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            gen_attention=gen_attention &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            (&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; gen_attention_blocks) &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    inplanes = planes * block.expansion&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, blocks):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        layers.append(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            block(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                inplanes=inplanes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                planes=planes,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                stride=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                dilation=dilation,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                style=style,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                with_cp=with_cp,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                conv_cfg=conv_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                norm_cfg=norm_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                dcn=dcn,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                gcb=gcb,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                gen_attention=gen_attention &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                (i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; gen_attention_blocks) &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; nn.Sequential(*layers)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到若stride不等于1或者输入的Feature map的channel不等于64 × expansion（BasicBlock.expansion = 1，Bottleneck.expansion = 4），则downsample设置成1x1x（64 × expansion），在可视化部分如图所示，downsample便指的是这一块的卷积层。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pic1.superbed.cn/item/5df0a40a1f8f59f4d6017ef0.jpg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;img src=&quot;https://pic1.superbed.cn/item/5df0a40a1f8f59f4d6017ef0&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;shortcut的操作（恒等映射）在&lt;code&gt;BasicBlock&lt;/code&gt;和&lt;code&gt;Bottleneck&lt;/code&gt;的&lt;code&gt;forward函数&lt;/code&gt;中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;class BasicBlock(nn.Module):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    identity = x&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.conv1(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.norm1(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.relu(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.conv2(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.norm2(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.downsample &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        identity = self.downsample(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out += identity&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.relu(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; out&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;class Bottleneck(nn.Module):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;_inner_forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        identity = x&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.conv1(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.norm1(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.relu(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; self.with_dcn:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            out = self.conv2(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;elif&lt;/span&gt; self.with_modulated_dcn:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            offset_mask = self.conv2_offset(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            offset = offset_mask[:, :&lt;span class=&quot;number&quot;&gt;18&lt;/span&gt;, :, :]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            mask = offset_mask[:, &lt;span class=&quot;number&quot;&gt;-9&lt;/span&gt;:, :, :].sigmoid()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            out = self.conv2(out, offset, mask)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            offset = self.conv2_offset(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            out = self.conv2(out, offset)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.norm2(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.relu(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_gen_attention:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            out = self.gen_attention_block(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.conv3(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = self.norm3(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_gcb:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            out = self.context_block(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.downsample &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            identity = self.downsample(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out += identity&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; out&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_cp &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; x.requires_grad:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = cp.checkpoint(_inner_forward, x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out = _inner_forward(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out = self.relu(out)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; out&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;需要注意的是在每个stage，只有第一次才会传入&lt;code&gt;downsample&lt;/code&gt;，之后是不传入的。这和ResNet的可视化结果一致。ResNet-50第一个stage也确实是堆叠了3个&lt;code&gt;Bottleneck&lt;/code&gt;，与代码中也一致。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pic.superbed.cn/item/5df0a7771f8f59f4d6095d68.jpg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;img src=&quot;https://pic.superbed.cn/item/5df0a7771f8f59f4d6095d68&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此外还需要说明的是在每个stage之后channel数会发生变化。&lt;/p&gt;
&lt;p&gt;另外，在&lt;code&gt;resnet.py&lt;/code&gt;中还有&lt;code&gt;init_weights&lt;/code&gt;，&lt;code&gt;forward&lt;/code&gt;，&lt;code&gt;train&lt;/code&gt;函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;init_weights&lt;/code&gt;在&lt;code&gt;two_stage.py&lt;/code&gt;被调用，将预训练权重加载到backbone中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;forward&lt;/code&gt;函数则是在&lt;code&gt;two_stage.py&lt;/code&gt;中的&lt;code&gt;extract_feat(img)&lt;/code&gt;中通过&lt;code&gt;self.backbone(img)&lt;/code&gt;调用，这是因为ResNet类继承至nn.Module，而在nn.Module中实现了&lt;code&gt;__call__&lt;/code&gt;，在nn.Module的&lt;code&gt;__call__&lt;/code&gt;中&lt;code&gt;result = self.forward(*input, **kwargs)&lt;/code&gt;，因此可以通过&lt;code&gt;self.backbone(img)&lt;/code&gt;直接调用&lt;code&gt;self.backbone.forward(img)&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本篇博客中，我们详细介绍了mmDetection框架中backbone的构建，此处以ResNet为例，结合论文、caffe的prototxt的可视化和代码，一起讲述了ResNet网络的构建过程。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Faster-R-CNN模块解读（二）-Backbone的构建&quot;&gt;&lt;a href=&quot;#Faster-R-CNN模块解读（二）-Backbone的构建&quot; class=&quot;headerlink&quot; title=&quot;Faster R-CNN模块解读（二） Backbone的构
    
    </summary>
    
      <category term="mmDetection源码分析" scheme="http://chr10003566.github.io./categories/mmDetection%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>mmDetection源码分析（三）：Faster R-CNN模块解读（一）</title>
    <link href="http://chr10003566.github.io./2019/12/10/mmdetection(3)/"/>
    <id>http://chr10003566.github.io./2019/12/10/mmdetection(3)/</id>
    <published>2019-12-10T11:09:14.000Z</published>
    <updated>2019-12-10T13:42:16.000Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Faster-R-CNN模块解读（一）—-检测器的构建&quot;&gt;&lt;a href=&quot;#Faster-R-CNN模块解读（一）—-检测器的构建&quot; class=&quot;headerlink&quot; title=&quot;Faster R-CNN模块解读（一）— 检测器的构建&quot;&gt;&lt;/a&gt;Faster R-CNN模块解读（一）— 检测器的构建&lt;/h1&gt;&lt;p&gt;根据之前的介绍，config文件中的 &lt;code&gt;model&lt;/code&gt; 中的 &lt;code&gt;type&lt;/code&gt; 指定了检测器是一个Faster R-CNN检测器。&lt;/p&gt;
&lt;p&gt;我们知道要调用一个检测模型要用到函数&lt;code&gt;model = build_detector(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)&lt;/code&gt;。接下来，我们看看其是如何调用到Faster R-CNN模块的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/configs/faster_rcnn_r50_fpn_1x.py&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;model = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    type=&lt;span class=&quot;string&quot;&gt;&#39;FasterRCNN&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    pretrained=&lt;span class=&quot;string&quot;&gt;&#39;torchvision://resnet50&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    backbone=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;ResNet&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        depth=&lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_stages=&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_indices=(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        frozen_stages=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        style=&lt;span class=&quot;string&quot;&gt;&#39;pytorch&#39;&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    neck=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;FPN&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=[&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;512&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1024&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2048&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_outs=&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rpn_head=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;RPNHead&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        feat_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_scales=[&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_ratios=[&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_strides=[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_means=[&lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_stds=[&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_cls=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;CrossEntropyLoss&#39;&lt;/span&gt;, use_sigmoid=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;, loss_weight=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_bbox=dict(type=&lt;span class=&quot;string&quot;&gt;&#39;SmoothL1Loss&#39;&lt;/span&gt;, beta=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; / &lt;span class=&quot;number&quot;&gt;9.0&lt;/span&gt;, loss_weight=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;)),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bbox_roi_extractor=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;SingleRoIExtractor&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        roi_layer=dict(type=&lt;span class=&quot;string&quot;&gt;&#39;RoIAlign&#39;&lt;/span&gt;, out_size=&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, sample_num=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        featmap_strides=[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;]),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bbox_head=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;SharedFCBBoxHead&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_fcs=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fc_out_channels=&lt;span class=&quot;number&quot;&gt;1024&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        roi_feat_size=&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_classes=&lt;span class=&quot;number&quot;&gt;81&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_means=[&lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_stds=[&lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        reg_class_agnostic=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_cls=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;CrossEntropyLoss&#39;&lt;/span&gt;, use_sigmoid=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;, loss_weight=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        loss_bbox=dict(type=&lt;span class=&quot;string&quot;&gt;&#39;SmoothL1Loss&#39;&lt;/span&gt;, beta=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, loss_weight=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;)))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/registry.py&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;BACKBONES = Registry(&lt;span class=&quot;string&quot;&gt;&#39;backbone&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NECKS = Registry(&lt;span class=&quot;string&quot;&gt;&#39;neck&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ROI_EXTRACTORS = Registry(&lt;span class=&quot;string&quot;&gt;&#39;roi_extractor&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SHARED_HEADS = Registry(&lt;span class=&quot;string&quot;&gt;&#39;shared_head&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;HEADS = Registry(&lt;span class=&quot;string&quot;&gt;&#39;head&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;LOSSES = Registry(&lt;span class=&quot;string&quot;&gt;&#39;loss&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;DETECTORS = Registry(&lt;span class=&quot;string&quot;&gt;&#39;detector&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/builder.py&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cfg, registry, default_args=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(cfg, list):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        modules = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            build_from_cfg(cfg_, registry, default_args) &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; cfg_ &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; nn.Sequential(*modules)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; build_from_cfg(cfg, registry, default_args)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_detector&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cfg, train_cfg=None, test_cfg=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/utils/registry.py&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_from_cfg&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cfg, registry, default_args=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;Build a module from config dict.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Args:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        cfg (dict): Config dict. It should at least contain the key &quot;type&quot;.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        registry (:obj:`Registry`): The registry to search the type from.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        default_args (dict, optional): Default initialization arguments.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Returns:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        obj: The constructed object.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; isinstance(cfg, dict) &lt;span class=&quot;keyword&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;type&#39;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;assert&lt;/span&gt; isinstance(default_args, dict) &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; default_args &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    args = cfg.copy()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    obj_type = args.pop(&lt;span class=&quot;string&quot;&gt;&#39;type&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; mmcv.is_str(obj_type):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        obj_type = registry.get(obj_type)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; obj_type &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; KeyError(&lt;span class=&quot;string&quot;&gt;&#39;&amp;#123;&amp;#125; is not in the &amp;#123;&amp;#125; registry&#39;&lt;/span&gt;.format(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                obj_type, registry.name))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; inspect.isclass(obj_type):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; TypeError(&lt;span class=&quot;string&quot;&gt;&#39;type must be a str or valid type, but got &amp;#123;&amp;#125;&#39;&lt;/span&gt;.format(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type(obj_type)))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; default_args &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; name, value &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; default_args.items():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            args.setdefault(name, value)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; obj_type(**args)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;通过&lt;code&gt;build_from_cfg&lt;/code&gt;返回了&lt;code&gt;cfg.model.type&lt;/code&gt;定义的类。registry指明了该类在&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/detectors&lt;/em&gt;&lt;/strong&gt;中，查看文件&lt;code&gt;faster_rcnn.py&lt;/code&gt;(此处有其框架设计的问题，比如注册器的使用，在本系列中暂时不分析)&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@DETECTORS.register_module&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;FasterRCNN&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(TwoStageDetector)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 backbone,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 rpn_head,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 bbox_roi_extractor,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 bbox_head,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 train_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 test_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 neck=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 shared_head=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 pretrained=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(FasterRCNN, self).__init__(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            backbone=backbone,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            neck=neck,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            shared_head=shared_head,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            rpn_head=rpn_head,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bbox_roi_extractor=bbox_roi_extractor,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bbox_head=bbox_head,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            train_cfg=train_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            test_cfg=test_cfg,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pretrained=pretrained)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到&lt;code&gt;FasterRCNN&lt;/code&gt;继承于&lt;code&gt;TwoStageDetector&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;mmdetection/mmdet/models/detectors/two_stage.py&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@DETECTORS.register_module&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TwoStageDetector&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(BaseDetector, RPNTestMixin, BBoxTestMixin,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                       MaskTestMixin)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 backbone,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 neck=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 shared_head=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 rpn_head=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 bbox_roi_extractor=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 bbox_head=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 mask_roi_extractor=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 mask_head=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 train_cfg=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 test_cfg=None,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 pretrained=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(TwoStageDetector, self).__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.backbone = builder.build_backbone(backbone)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; neck &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.neck = builder.build_neck(neck)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; shared_head &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.shared_head = builder.build_shared_head(shared_head)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; rpn_head &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.rpn_head = builder.build_head(rpn_head)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; bbox_head &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.bbox_roi_extractor = builder.build_roi_extractor(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                bbox_roi_extractor)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.bbox_head = builder.build_head(bbox_head)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; mask_head &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; mask_roi_extractor &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.mask_roi_extractor = builder.build_roi_extractor(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    mask_roi_extractor)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.share_roi_extractor = &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.share_roi_extractor = &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.mask_roi_extractor = self.bbox_roi_extractor&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.mask_head = builder.build_head(mask_head)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.train_cfg = train_cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.test_cfg = test_cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.init_weights(pretrained=pretrained)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, pretrained=None)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(TwoStageDetector, self).init_weights(pretrained)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.backbone.init_weights(pretrained=pretrained)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_neck:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(self.neck, nn.Sequential):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; m &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.neck:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    m.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.neck.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_shared_head:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.shared_head.init_weights(pretrained=pretrained)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_rpn:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.rpn_head.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_bbox:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.bbox_roi_extractor.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.bbox_head.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; self.with_mask:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.mask_head.init_weights()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; self.share_roi_extractor:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.mask_roi_extractor.init_weights()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到通过&lt;code&gt;builder&lt;/code&gt;将检测器的backbone，neck，shared_head，bbox_head，mask_head等全部创建了。并通过&lt;code&gt;init_weights&lt;/code&gt;初始话权重，后续有：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;extract_feat()&lt;/code&gt;&lt;br&gt;&lt;code&gt;forward_train()&lt;/code&gt;&lt;br&gt;&lt;code&gt;simple_test &amp;amp;&amp;amp; aug_test&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;下一篇blog将介绍Backbone的构建。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Faster-R-CNN模块解读（一）—-检测器的构建&quot;&gt;&lt;a href=&quot;#Faster-R-CNN模块解读（一）—-检测器的构建&quot; class=&quot;headerlink&quot; title=&quot;Faster R-CNN模块解读（一）— 检测器的构建&quot;&gt;&lt;/a&gt;Faste
    
    </summary>
    
      <category term="mmDetection源码分析" scheme="http://chr10003566.github.io./categories/mmDetection%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>CornerNet- Detecting Objects as Paired Keypoints论文解读</title>
    <link href="http://chr10003566.github.io./2019/12/09/%E5%AD%A6%E4%B9%A0%20CornerNet-%20Detecting%20Objects%20as%20Paired%20Keypoints/"/>
    <id>http://chr10003566.github.io./2019/12/09/学习 CornerNet- Detecting Objects as Paired Keypoints/</id>
    <published>2019-12-09T13:25:14.000Z</published>
    <updated>2020-02-16T08:19:15.390Z</updated>
    
    <content type="html">&lt;h1 id=&quot;CornerNet-Detecting-Objects-as-Paired-Keypoints论文笔记&quot;&gt;&lt;a href=&quot;#CornerNet-Detecting-Objects-as-Paired-Keypoints论文笔记&quot; class=&quot;headerlink&quot; title=&quot;CornerNet- Detecting Objects as Paired Keypoints论文笔记&quot;&gt;&lt;/a&gt;CornerNet- Detecting Objects as Paired Keypoints论文笔记&lt;/h1&gt;&lt;p&gt;论文：CornerNet: Detecting Objects as Paired Keypoints&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1808.01244&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1808.01244&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/umich-vl/CornerNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/umich-vl/CornerNet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇发表在ECCV2018上的目标检测文章给人一种眼前一亮的感觉，简单说一下几个比较吸引我的点：1、将目标检测问题当作关键点检测问题来解决，也就是通过检测目标框的左上角和右下角两个关键点得到预测框，因此CornerNet算法中没有anchor的概念，这种做法在目标检测领域是比较创新的而且能够取得不错效果是很难的。2、整个检测网络的训练是从头开始的，并不基于预训练的分类模型，这使得用户能够自由设计特征提取网络，不用受预训练模型的限制。&lt;/p&gt;
&lt;p&gt;我们知道目前大部分常用的目标检测算法都是基于anchor的，比如Faster RCNN系列，SSD，YOLO（v2、v3）等，引入anchor后检测效果提升确实比较明显（比如YOLO v1和YOLO v2），但是引入anchor的缺点在于：1、正负样本不均衡。大部分检测算法的anchor数量都成千上万，但是一张图中的目标数量并没有那么多，这就导致正样本数量会远远小于负样本，因此有了对负样本做欠采样以及focal loss等算法来解决这个问题。2、引入更多的超参数，比如anchor的数量、大小和宽高比等。因此这篇不采用anchor却能有不错效果的CornerNet就省去了这几个额外的操作，确实是非常有意思的作品。&lt;/p&gt;
&lt;p&gt;CornerNet算法整体结构如Figure4所示。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201602253?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;首先1个7×7的卷积层将输入图像尺寸缩小为原来的1/4（论文中输入图像大小是511×511，缩小后得到128×128大小的输出）。&lt;br&gt;然后经过特征提取网络（backbone network）提取特征，该网络采用hourglass network，该网络通过串联多个hourglass module组成（Figure4中的hourglass network由2个hourglass module组成），每个hourglass module都是先通过一系列的降采样操作缩小输入的大小，然后通过上采样恢复到输入图像大小，因此该部分的输出特征图大小还是128×128，整个hourglass network的深度是104层。&lt;br&gt;hourglass module后会有两个输出分支模块，分别表示左上角点预测分支和右下角点预测分支，每个分支模块包含一个corner pooling层和3个输出：heatmaps、embeddings和offsets。heatmaps是输出预测角点信息，可以用维度为CHW的特征图表示，其中C表示目标的类别（注意：没有背景类），这个特征图的每个通道都是一个mask，mask的每个值（范围为0到1，论文中写的该mask是binary mask，也就是0或1，个人感觉是笔误，预测值应该是0到1，否则后面公式1计算损失函数时就没有意思了）表示该点是角点的分数；embeddings用来对预测的corner点做group，也就是找到属于同一个目标的左上角角点和右下角角点；offsets用来对预测框做微调，这是因为从输入图像中的点映射到特征图时有量化误差，offsets就是用来输出这些误差信息。&lt;/p&gt;
&lt;p&gt;接下来分别看看这3个输出和corner pooling的含义。&lt;/p&gt;
&lt;p&gt;CornerNet的第一个输出是headmaps，也就是预测角点的位置。&lt;br&gt;公式1是针对角点预测（headmaps）的损失函数，整体上是改良版的focal loss。几个参数的含义：pcij表示预测的heatmaps在第c个通道（类别c）的(i,j)位置的值，ycij表示对应位置的ground truth，N表示目标的数量。ycij=1时候的损失函数容易理解，就是focal loss，α参数用来控制难易分类样本的损失权重；ycij等于其他值时表示(i,j)点不是类别c的目标角点，照理说此时ycij应该是0（大部分算法都是这样处理的），但是这里ycij不是0，而是用基于ground truth角点的高斯分布计算得到，因此距离ground truth比较近的(i,j)点的ycij值接近1，这部分通过β参数控制权重，这是和focal loss的差别。为什么对不同的负样本点用不同权重的损失函数呢？这是因为靠近ground truth的误检角点组成的预测框仍会和ground truth有较大的重叠面积，如Figure5所示。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201637309?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Figure5是关于对不同负样本点的损失函数采取不同权重值的原因。红色实线框是ground truth；橘色圆圈是根据ground truth的左上角角点、右下角角点和设定的半径值画出来的，半径是根据圆圈内的角点组成的框和ground truth的IOU值大于0.7而设定的，圆圈内的点的数值是以圆心往外呈二维的高斯分布；白色虚线是一个预测框，可以看出这个预测框的两个角点和ground truth并不重合，但是该预测框基本框住了目标，因此是有用的预测框，所以要有一定权重的损失返回，这就是为什么要对不同负样本点的损失函数采取不同权重值的原因。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/2018101220165774?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;CornerNet的第二个输出是offset，这个值和目标检测算法中预测的offset类似却完全不一样，说类似是因为都是偏置信息，说不一样是因为在目标检测算法中预测的offset是表示预测框和anchor之间的偏置，而这里的offset是表示在取整计算时丢失的精度信息，也就是公式2所表达的内容。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201718217?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;我们知道从输入图像到特征图之间会有尺寸缩小，假设缩小倍数是n，那么输入图像上的(x,y)点对应到特征图上就如下式子&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201733925?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;式子中的符号是向下取整，取整会带来精度丢失，这尤其影响小尺寸目标的回归，Faster RCNN中的 ROI Pooling也是有类似的精度丢失问题。所以通过公式2计算offset，然后通过公式3的smooth L1损失函数监督学习该参数，和常见的目标检测算法中的回归支路类似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201749649?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;CornerNet的第三个输出是embedding，对应文章中group corner的内容。前面介绍了关于角点的检测，在那部分中对角点的预测都是独立的，不涉及一个目标的一对角点的概念，因此如何找到一个目标的两个角点就是第三个输出embedding做的工作。这部分是受associative embedding那篇文章的启发，简而言之就是基于不同角点的embedding vector之间的距离找到每个目标的一对角点，如果一个左上角角点和一个右下角角点属于同一个目标，那么二者的embedding vector之间的距离应该很小。&lt;/p&gt;
&lt;p&gt;embedding这部分的训练是通过两个损失函数实现的，etk表示第k个目标的左上角角点的embedding vector，ebk表示第k个目标的右下角角点的embedding vector，ek表示etk和ebk的均值。公式4用来缩小属于同一个目标（第k个目标）的两个角点的embedding vector（etk和ebk）距离。公式5用来扩大不属于同一个目标的两个角点的embedding vector距离。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201831690?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后一部分是corner pooling。首先看看为什么要引入corner pooling，如图Figure2所示。因为CornerNet是预测左上角和右下角两个角点，但是这两个角点在不同目标上没有相同规律可循，如果采用普通池化操作，那么在训练预测角点支路时会比较困难。考虑到左上角角点的右边有目标顶端的特征信息（第一张图的头顶），左上角角点的下边有目标左侧的特征信息（第一张图的手），因此如果左上角角点经过池化操作后能有这两个信息，那么就有利于该点的预测，这就有了corner pooling。&lt;/p&gt;
&lt;p&gt;Figure3是针对左上角点做corner pooling的示意图，该层有2个输入特征图，特征图的宽高分别用W和H表示，假设接下来要对图中红色点（坐标假设是(i,j)）做corner pooling，那么就计算(i,j)到(i,H)的最大值（对应Figure3上面第二个图），类似于找到Figure2中第一张图的左侧手信息；同时计算(i,j)到(W,j)的最大值（对应Figure3下面第二个图），类似于找到Figure2中第一张图的头顶信息，然后将这两个最大值相加得到(i,j)点的值（对应Figure3最后一个图的蓝色点）。右下角点的corner pooling操作类似，只不过计算最大值变成从(0,j)到(i,j)和从(i,0)到(i,j)。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/2018101220185287?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Figure6也是针对左上角点做corner pooling的示意图，是Figure3的具体数值计算例子，该图一共计算了4个点的corner pooling结果。第二列的数值计算和Figure3介绍的一样，比如第一行第一个图中的0值点，计算该点的最大值时是计算该点和其右侧的值为2的点的最大值，因此得到的就是2。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201913296?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Figure7是Figure4中预测模块的详细结构，该结构包括corner pooling模块和预测输出模块两部分，corner pooling模块采用了类似residual block的形式，有一个skip connection，虚线框部分执行的就是corner pooling操作，也就是Figure6的操作，这样整个corner pooling操作就介绍完了。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201934254?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;模型测试时候的几个细节：1、在得到预测角点后，会对这些角点做NMS操作，选择前100个左上角角点和100个右下角角点。2、计算左上角和右下角角点的embedding vector的距离时采用L1范数，距离大于0.5或者两个点来自不同类别的目标的都不能构成一对。3、测试图像采用0值填充方式得到指定大小作为网络的输入，而不是采用resize，另外同时测试图像的水平翻转图并融合二者的结果。4、最后通过soft-nms操作去除冗余框，只保留前100个预测框。&lt;/p&gt;
&lt;p&gt;实验结果：&lt;br&gt;Table1是关于conner pooling的对比实验，可以看出添加conner pooling（第二行）对效果的提升比较明显，这种提升尤其在目标尺度比较大的数据中表现明显。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012201953558?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;Table2是关于对不同位置负样本采取不同权重的损失函数的效果。第一行是不采用这种策略的效果；第二行是采用固定半径值的效果，可以看出提升比较明显；第三行是采用基于目标计算得到的半径值的效果（这篇文章所采用的），效果得到进一步提升。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/2018101220211329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Table3是关于这篇文章的几个创新点效果。第一行是这篇文章的算法结果；第二行是将角点预测结果用ground truth代替，可以看到提升非常大；第三行是进一步将偏置用ground truth代替，相比之下提升会小很多。这说明目前该算法的瓶颈主要在于角点预测。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012202130999?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;br&gt;Table4是connerNet和其他目标检测算法的效果对比，可以看出模型效果还是很不错的。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20181012202146342?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQzODAxNjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;整篇文章看下来收获还是比较多的，希望目标检测领域能够继续百花齐放。不过比较好奇算法的速度，在实验部分没有看到对比。&lt;br&gt;————————————————&lt;br&gt;版权声明：本文为CSDN博主「AI之路」的原创文章。&lt;br&gt;原文链接：&lt;a href=&quot;https://blog.csdn.net/u014380165/article/details/83032273&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AI之路-CornerNet&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CornerNet-Detecting-Objects-as-Paired-Keypoints论文笔记&quot;&gt;&lt;a href=&quot;#CornerNet-Detecting-Objects-as-Paired-Keypoints论文笔记&quot; class=&quot;headerlin
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>mmDetection源码分析（2）：训练与配置文件</title>
    <link href="http://chr10003566.github.io./2019/12/03/mmdetection(2)/"/>
    <id>http://chr10003566.github.io./2019/12/03/mmdetection(2)/</id>
    <published>2019-12-03T14:22:22.000Z</published>
    <updated>2019-12-03T14:26:20.260Z</updated>
    
    <content type="html">&lt;h1 id=&quot;训练函数调用&quot;&gt;&lt;a href=&quot;#训练函数调用&quot; class=&quot;headerlink&quot; title=&quot;训练函数调用&quot;&gt;&lt;/a&gt;训练函数调用&lt;/h1&gt;&lt;h2 id=&quot;训练脚本&quot;&gt;&lt;a href=&quot;#训练脚本&quot; class=&quot;headerlink&quot; title=&quot;训练脚本&quot;&gt;&lt;/a&gt;训练脚本&lt;/h2&gt;&lt;p&gt;&lt;code&gt;python tools/train.py configs/faster_rcnn_r50_fpn_1x.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;训练可选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;–work_dir：模型checkpoint以及训练log文件的输出目录，若在脚本中不设置，则为&lt;code&gt;config/*.py&lt;/code&gt;中work_dir中的路径。&lt;/li&gt;
&lt;li&gt;–resume_from：指定在某个checkpoint的基础上继续训练，若在脚本中不设置，则为&lt;code&gt;config/*.py&lt;/code&gt;中resume_from中的值，默认为None。&lt;/li&gt;
&lt;li&gt;–validate：指是否在训练中建立checkpoint的时候对该checkpoint进行评估&lt;/li&gt;
&lt;li&gt;–gpus: 指使用GPU的数量，默认值为1&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;build-detector&quot;&gt;&lt;a href=&quot;#build-detector&quot; class=&quot;headerlink&quot; title=&quot;build_detector&quot;&gt;&lt;/a&gt;build_detector&lt;/h2&gt;&lt;h2 id=&quot;build-dataset&quot;&gt;&lt;a href=&quot;#build-dataset&quot; class=&quot;headerlink&quot; title=&quot;build_dataset&quot;&gt;&lt;/a&gt;build_dataset&lt;/h2&gt;&lt;h2 id=&quot;train-detector&quot;&gt;&lt;a href=&quot;#train-detector&quot; class=&quot;headerlink&quot; title=&quot;train_detector&quot;&gt;&lt;/a&gt;train_detector&lt;/h2&gt;&lt;p&gt;train_detector三个输入参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model：build_detector返回的模型&lt;/li&gt;
&lt;li&gt;train_dataset：build_dataset返回的dataset&lt;/li&gt;
&lt;li&gt;cfg：模型的配置文件&lt;/li&gt;
&lt;li&gt;distributed：默认为False&lt;/li&gt;
&lt;li&gt;validate：默认为False&lt;/li&gt;
&lt;li&gt;logger：默认为None，若为None则会重新创建&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;配置文件详解&quot;&gt;&lt;a href=&quot;#配置文件详解&quot; class=&quot;headerlink&quot; title=&quot;配置文件详解&quot;&gt;&lt;/a&gt;配置文件详解&lt;/h1&gt;&lt;h2 id=&quot;model&quot;&gt;&lt;a href=&quot;#model&quot; class=&quot;headerlink&quot; title=&quot;model&quot;&gt;&lt;/a&gt;model&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;type：model类型，需要在&lt;code&gt;/mmdet/models/detectors/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;li&gt;pretrained：预训练的模型&lt;/li&gt;
&lt;li&gt;backbone: 骨架参数字典  types需要在&lt;code&gt;/mmdet/models/backbones/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;li&gt;neck:neck的类型，需要在&lt;code&gt;/mmdet/models/necks/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;li&gt;rpn_head: RPN的类型需要在&lt;code&gt;/mmdet/models/anchor_heads/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;li&gt;bbox_roi_extractor: 目前只有&lt;code&gt;SingleRoIExtractor&lt;/code&gt;这一种类型，需在&lt;code&gt;/mmdet/models/roi_extractors/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;li&gt;bbox_head: head的类型需要在&lt;code&gt;/mmdet/models/bbox_heads/__init__.py&lt;/code&gt;中定义&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;train-cfg&quot;&gt;&lt;a href=&quot;#train-cfg&quot; class=&quot;headerlink&quot; title=&quot;train_cfg&quot;&gt;&lt;/a&gt;train_cfg&lt;/h2&gt;&lt;p&gt;训练超参数的设置。需要设置一下参数字典rpn，rcnn。&lt;br&gt;其中assigner和sampler的类型分别要在&lt;code&gt;/mmdet/core/box/assigners/__init__.py&lt;/code&gt;和&lt;code&gt;/mmdet/core/box/samplers/__init__.py&lt;/code&gt;中进行配置。&lt;/p&gt;
&lt;h2 id=&quot;test-cfg&quot;&gt;&lt;a href=&quot;#test-cfg&quot; class=&quot;headerlink&quot; title=&quot;test_cfg&quot;&gt;&lt;/a&gt;test_cfg&lt;/h2&gt;&lt;p&gt;测试阶段的超参数的配置&lt;/p&gt;
&lt;p&gt;需要设置rpn、rcnn的参数字典，要配置如nms的阈值与置信度的阈值。&lt;/p&gt;
&lt;h2 id=&quot;data&quot;&gt;&lt;a href=&quot;#data&quot; class=&quot;headerlink&quot; title=&quot;data&quot;&gt;&lt;/a&gt;data&lt;/h2&gt;&lt;p&gt;dataset_type：数据集类型，需要在&lt;code&gt;/mmdet/datasets/__init__.py&lt;/code&gt;中设置数据集类型。&lt;br&gt;data_root：数据集根目录&lt;/p&gt;
&lt;p&gt;参数字典data配置，配置train、val、test数据集的参数。&lt;/p&gt;
&lt;h2 id=&quot;optimizer&quot;&gt;&lt;a href=&quot;#optimizer&quot; class=&quot;headerlink&quot; title=&quot;optimizer&quot;&gt;&lt;/a&gt;optimizer&lt;/h2&gt;&lt;p&gt;优化器的选择。&lt;/p&gt;
&lt;p&gt;通过设置参数，构造优化器。&lt;/p&gt;
&lt;h2 id=&quot;lr-config&quot;&gt;&lt;a href=&quot;#lr-config&quot; class=&quot;headerlink&quot; title=&quot;lr_config&quot;&gt;&lt;/a&gt;lr_config&lt;/h2&gt;&lt;p&gt;学习率优化策略参数设置。&lt;/p&gt;
&lt;h2 id=&quot;checkpoint-config&quot;&gt;&lt;a href=&quot;#checkpoint-config&quot; class=&quot;headerlink&quot; title=&quot;checkpoint_config&quot;&gt;&lt;/a&gt;checkpoint_config&lt;/h2&gt;&lt;p&gt;设置多少epoch存储一次checkpoint&lt;/p&gt;
&lt;h2 id=&quot;log-config&quot;&gt;&lt;a href=&quot;#log-config&quot; class=&quot;headerlink&quot; title=&quot;log_config&quot;&gt;&lt;/a&gt;log_config&lt;/h2&gt;&lt;p&gt;输出log文件的配置&lt;br&gt;多少个batch输出一次信息&lt;br&gt;log文件的风格&lt;/p&gt;
&lt;h2 id=&quot;其他&quot;&gt;&lt;a href=&quot;#其他&quot; class=&quot;headerlink&quot; title=&quot;其他&quot;&gt;&lt;/a&gt;其他&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;total_epoches: 最大epoch数&lt;/li&gt;
&lt;li&gt;log_level:&lt;/li&gt;
&lt;li&gt;work_dir：log文件和模型文件存储路径&lt;/li&gt;
&lt;li&gt;load_from：训练模型的路径&lt;/li&gt;
&lt;li&gt;resume_from：恢复训练模型的路径&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下内容从&lt;a href=&quot;https://blog.csdn.net/hajlyx/article/details/85991400&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;mmdetection的configs中的各项参数具体解释&lt;/a&gt;转载过来。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;90&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;91&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;92&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;94&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;95&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;96&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;98&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;99&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;100&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;101&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;102&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;103&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;104&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;105&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;106&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;107&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;108&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;109&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;110&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;111&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;112&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;113&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;114&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;115&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;116&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;117&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;118&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;119&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;120&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;121&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;122&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;123&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;124&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;125&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;126&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;127&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;128&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;129&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;130&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;131&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;132&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;133&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;134&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;135&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;136&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;137&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;138&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;139&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;140&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;141&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;142&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;143&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;144&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;145&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;146&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;147&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;148&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;149&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;150&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;151&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;152&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;153&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;155&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;156&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# model settings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;model = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	type=&lt;span class=&quot;string&quot;&gt;&#39;FasterRCNN&#39;&lt;/span&gt;,                         &lt;span class=&quot;comment&quot;&gt;# model类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    pretrained=&lt;span class=&quot;string&quot;&gt;&#39;modelzoo://resnet50&#39;&lt;/span&gt;,          &lt;span class=&quot;comment&quot;&gt;# 预训练模型：imagenet-resnet50&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    backbone=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;ResNet&#39;&lt;/span&gt;,                         &lt;span class=&quot;comment&quot;&gt;# backbone类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        depth=&lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;,                              &lt;span class=&quot;comment&quot;&gt;# 网络层数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_stages=&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,                          &lt;span class=&quot;comment&quot;&gt;# resnet的stage数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_indices=(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;),              &lt;span class=&quot;comment&quot;&gt;# 输出的stage的序号&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        frozen_stages=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,                       &lt;span class=&quot;comment&quot;&gt;# 冻结的stage数量，即该stage不更新参数，-1表示所有的stage都更新参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        style=&lt;span class=&quot;string&quot;&gt;&#39;pytorch&#39;&lt;/span&gt;),                      &lt;span class=&quot;comment&quot;&gt;# 网络风格：如果设置pytorch，则stride为2的层是conv3x3的卷积层；如果设置caffe，则stride为2的层是第一个conv1x1的卷积层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    neck=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;FPN&#39;&lt;/span&gt;,                            &lt;span class=&quot;comment&quot;&gt;# neck类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=[&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;512&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1024&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2048&lt;/span&gt;],    &lt;span class=&quot;comment&quot;&gt;# 输入的各个stage的通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                      &lt;span class=&quot;comment&quot;&gt;# 输出的特征层的通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_outs=&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;),                           &lt;span class=&quot;comment&quot;&gt;# 输出的特征层的数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rpn_head=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;RPNHead&#39;&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# RPN网络类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                       &lt;span class=&quot;comment&quot;&gt;# RPN网络的输入通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        feat_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                     &lt;span class=&quot;comment&quot;&gt;# 特征层的通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_scales=[&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;],                     &lt;span class=&quot;comment&quot;&gt;# 生成的anchor的baselen，baselen = sqrt(w*h)，w和h为anchor的宽和高&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_ratios=[&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt;],         &lt;span class=&quot;comment&quot;&gt;# anchor的宽高比&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        anchor_strides=[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;64&lt;/span&gt;],     &lt;span class=&quot;comment&quot;&gt;# 在每个特征层上的anchor的步长（对应于原图）&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_means=[&lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;],         &lt;span class=&quot;comment&quot;&gt;# 均值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_stds=[&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;],      &lt;span class=&quot;comment&quot;&gt;# 方差&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        use_sigmoid_cls=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;),                 &lt;span class=&quot;comment&quot;&gt;# 是否使用sigmoid来进行分类，如果False则使用softmax来分类&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bbox_roi_extractor=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;SingleRoIExtractor&#39;&lt;/span&gt;,                                   &lt;span class=&quot;comment&quot;&gt;# RoIExtractor类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        roi_layer=dict(type=&lt;span class=&quot;string&quot;&gt;&#39;RoIAlign&#39;&lt;/span&gt;, out_size=&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, sample_num=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;),   &lt;span class=&quot;comment&quot;&gt;# ROI具体参数：ROI类型为ROIalign，输出尺寸为7，sample数为2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        out_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                                            &lt;span class=&quot;comment&quot;&gt;# 输出通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        featmap_strides=[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;]),                             &lt;span class=&quot;comment&quot;&gt;# 特征图的步长&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bbox_head=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=&lt;span class=&quot;string&quot;&gt;&#39;SharedFCBBoxHead&#39;&lt;/span&gt;,                     &lt;span class=&quot;comment&quot;&gt;# 全连接层类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_fcs=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,                                   &lt;span class=&quot;comment&quot;&gt;# 全连接层数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        in_channels=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                             &lt;span class=&quot;comment&quot;&gt;# 输入通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fc_out_channels=&lt;span class=&quot;number&quot;&gt;1024&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# 输出通道数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        roi_feat_size=&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,                             &lt;span class=&quot;comment&quot;&gt;# ROI特征层尺寸&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        num_classes=&lt;span class=&quot;number&quot;&gt;81&lt;/span&gt;,                              &lt;span class=&quot;comment&quot;&gt;# 分类器的类别数量+1，+1是因为多了一个背景的类别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_means=[&lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.&lt;/span&gt;],               &lt;span class=&quot;comment&quot;&gt;# 均值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        target_stds=[&lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;],            &lt;span class=&quot;comment&quot;&gt;# 方差&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        reg_class_agnostic=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;))                   &lt;span class=&quot;comment&quot;&gt;# 是否采用class_agnostic的方式来预测，class_agnostic表示输出bbox时只考虑其是否为前景，后续分类的时候再根据该bbox在网络中的类别得分来分类，也就是说一个框可以对应多个类别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# model training and testing settings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;train_cfg = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rpn=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        assigner=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;MaxIoUAssigner&#39;&lt;/span&gt;,            &lt;span class=&quot;comment&quot;&gt;# RPN网络的正负样本划分&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pos_iou_thr=&lt;span class=&quot;number&quot;&gt;0.7&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 正样本的iou阈值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            neg_iou_thr=&lt;span class=&quot;number&quot;&gt;0.3&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 负样本的iou阈值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            min_pos_iou=&lt;span class=&quot;number&quot;&gt;0.3&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ignore_iof_thr=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;),               &lt;span class=&quot;comment&quot;&gt;# 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        sampler=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;RandomSampler&#39;&lt;/span&gt;,             &lt;span class=&quot;comment&quot;&gt;# 正负样本提取器类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            num=&lt;span class=&quot;number&quot;&gt;256&lt;/span&gt;,                          &lt;span class=&quot;comment&quot;&gt;# 需提取的正负样本数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pos_fraction=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;,                 &lt;span class=&quot;comment&quot;&gt;# 正样本比例&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            neg_pos_ub=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,                    &lt;span class=&quot;comment&quot;&gt;# 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            add_gt_as_proposals=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;),       &lt;span class=&quot;comment&quot;&gt;# 把ground truth加入proposal作为正样本&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        allowed_border=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,                     &lt;span class=&quot;comment&quot;&gt;# 允许在bbox周围外扩一定的像素&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        pos_weight=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# 正样本权重，-1表示不改变原始的权重&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        smoothl1_beta=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; / &lt;span class=&quot;number&quot;&gt;9.0&lt;/span&gt;,                &lt;span class=&quot;comment&quot;&gt;# 平滑L1系数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        debug=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;),                         &lt;span class=&quot;comment&quot;&gt;# debug模式&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rcnn=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        assigner=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;MaxIoUAssigner&#39;&lt;/span&gt;,            &lt;span class=&quot;comment&quot;&gt;# RCNN网络正负样本划分&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pos_iou_thr=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 正样本的iou阈值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            neg_iou_thr=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 负样本的iou阈值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            min_pos_iou=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;,                  &lt;span class=&quot;comment&quot;&gt;# 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ignore_iof_thr=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;),               &lt;span class=&quot;comment&quot;&gt;# 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        sampler=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            type=&lt;span class=&quot;string&quot;&gt;&#39;RandomSampler&#39;&lt;/span&gt;,             &lt;span class=&quot;comment&quot;&gt;# 正负样本提取器类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            num=&lt;span class=&quot;number&quot;&gt;512&lt;/span&gt;,                          &lt;span class=&quot;comment&quot;&gt;# 需提取的正负样本数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            pos_fraction=&lt;span class=&quot;number&quot;&gt;0.25&lt;/span&gt;,                &lt;span class=&quot;comment&quot;&gt;# 正样本比例&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            neg_pos_ub=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,                    &lt;span class=&quot;comment&quot;&gt;# 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            add_gt_as_proposals=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;),        &lt;span class=&quot;comment&quot;&gt;# 把ground truth加入proposal作为正样本&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        pos_weight=&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# 正样本权重，-1表示不改变原始的权重&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        debug=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;))                         &lt;span class=&quot;comment&quot;&gt;# debug模式&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;test_cfg = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rpn=dict(                                 &lt;span class=&quot;comment&quot;&gt;# 推断时的RPN参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        nms_across_levels=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,              &lt;span class=&quot;comment&quot;&gt;# 在所有的fpn层内做nms&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        nms_pre=&lt;span class=&quot;number&quot;&gt;2000&lt;/span&gt;,                         &lt;span class=&quot;comment&quot;&gt;# 在nms之前保留的的得分最高的proposal数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        nms_post=&lt;span class=&quot;number&quot;&gt;2000&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# 在nms之后保留的的得分最高的proposal数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        max_num=&lt;span class=&quot;number&quot;&gt;2000&lt;/span&gt;,                         &lt;span class=&quot;comment&quot;&gt;# 在后处理完成之后保留的proposal数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        nms_thr=&lt;span class=&quot;number&quot;&gt;0.7&lt;/span&gt;,                          &lt;span class=&quot;comment&quot;&gt;# nms阈值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        min_bbox_size=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;),                     &lt;span class=&quot;comment&quot;&gt;# 最小bbox尺寸&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    rcnn=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        score_thr=&lt;span class=&quot;number&quot;&gt;0.05&lt;/span&gt;, nms=dict(type=&lt;span class=&quot;string&quot;&gt;&#39;nms&#39;&lt;/span&gt;, iou_thr=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;), max_per_img=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;)   &lt;span class=&quot;comment&quot;&gt;# max_per_img表示最终输出的det bbox数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# soft-nms is also supported for rcnn testing&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# e.g., nms=dict(type=&#39;soft_nms&#39;, iou_thr=0.5, min_score=0.05)            # soft_nms参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# dataset settings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;dataset_type = &lt;span class=&quot;string&quot;&gt;&#39;CocoDataset&#39;&lt;/span&gt;                &lt;span class=&quot;comment&quot;&gt;# 数据集类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;data_root = &lt;span class=&quot;string&quot;&gt;&#39;data/coco/&#39;&lt;/span&gt;                    &lt;span class=&quot;comment&quot;&gt;# 数据集根目录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img_norm_cfg = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    mean=[&lt;span class=&quot;number&quot;&gt;123.675&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;116.28&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;103.53&lt;/span&gt;], std=[&lt;span class=&quot;number&quot;&gt;58.395&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;57.12&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;57.375&lt;/span&gt;], to_rgb=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)   &lt;span class=&quot;comment&quot;&gt;# 输入图像初始化，减去均值mean并处以方差std，to_rgb表示将bgr转为rgb&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;data = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    imgs_per_gpu=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,                &lt;span class=&quot;comment&quot;&gt;# 每个gpu计算的图像数量&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    workers_per_gpu=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,             &lt;span class=&quot;comment&quot;&gt;# 每个gpu分配的线程数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    train=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=dataset_type,                                                 &lt;span class=&quot;comment&quot;&gt;# 数据集类型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ann_file=data_root + &lt;span class=&quot;string&quot;&gt;&#39;annotations/instances_train2017.json&#39;&lt;/span&gt;,       &lt;span class=&quot;comment&quot;&gt;# 数据集annotation路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_prefix=data_root + &lt;span class=&quot;string&quot;&gt;&#39;train2017/&#39;&lt;/span&gt;,                               &lt;span class=&quot;comment&quot;&gt;# 数据集的图片路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_scale=(&lt;span class=&quot;number&quot;&gt;1333&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;800&lt;/span&gt;),                                             &lt;span class=&quot;comment&quot;&gt;# 输入图像尺寸，最大边1333，最小边800&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_norm_cfg=img_norm_cfg,                                         &lt;span class=&quot;comment&quot;&gt;# 图像初始化参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        size_divisor=&lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 对图像进行resize时的最小单位，32表示所有的图像都会被resize成32的倍数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        flip_ratio=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;,                                                    &lt;span class=&quot;comment&quot;&gt;# 图像的随机左右翻转的概率&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_mask=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 训练时附带mask&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_crowd=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 训练时附带difficult的样本&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_label=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;),                                                  &lt;span class=&quot;comment&quot;&gt;# 训练时附带label&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    val=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=dataset_type,                                                 &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ann_file=data_root + &lt;span class=&quot;string&quot;&gt;&#39;annotations/instances_val2017.json&#39;&lt;/span&gt;,         &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_prefix=data_root + &lt;span class=&quot;string&quot;&gt;&#39;val2017/&#39;&lt;/span&gt;,                                 &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_scale=(&lt;span class=&quot;number&quot;&gt;1333&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;800&lt;/span&gt;),                                             &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_norm_cfg=img_norm_cfg,                                         &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        size_divisor=&lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        flip_ratio=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,                                                      &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_mask=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_crowd=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_label=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;),                                                  &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    test=dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        type=dataset_type,                                                 &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ann_file=data_root + &lt;span class=&quot;string&quot;&gt;&#39;annotations/instances_val2017.json&#39;&lt;/span&gt;,         &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_prefix=data_root + &lt;span class=&quot;string&quot;&gt;&#39;val2017/&#39;&lt;/span&gt;,                                 &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_scale=(&lt;span class=&quot;number&quot;&gt;1333&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;800&lt;/span&gt;),                                             &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        img_norm_cfg=img_norm_cfg,                                         &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        size_divisor=&lt;span class=&quot;number&quot;&gt;32&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        flip_ratio=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,                                                      &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_mask=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        with_label=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,                                                  &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        test_mode=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;))                                                   &lt;span class=&quot;comment&quot;&gt;# 同上&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# optimizer&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;optimizer = dict(type=&lt;span class=&quot;string&quot;&gt;&#39;SGD&#39;&lt;/span&gt;, lr=&lt;span class=&quot;number&quot;&gt;0.02&lt;/span&gt;, momentum=&lt;span class=&quot;number&quot;&gt;0.9&lt;/span&gt;, weight_decay=&lt;span class=&quot;number&quot;&gt;0.0001&lt;/span&gt;)   &lt;span class=&quot;comment&quot;&gt;# 优化参数，lr为学习率，momentum为动量因子，weight_decay为权重衰减因子&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;optimizer_config = dict(grad_clip=dict(max_norm=&lt;span class=&quot;number&quot;&gt;35&lt;/span&gt;, norm_type=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;))          &lt;span class=&quot;comment&quot;&gt;# 梯度均衡参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# learning policy&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lr_config = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    policy=&lt;span class=&quot;string&quot;&gt;&#39;step&#39;&lt;/span&gt;,                        &lt;span class=&quot;comment&quot;&gt;# 优化策略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    warmup=&lt;span class=&quot;string&quot;&gt;&#39;linear&#39;&lt;/span&gt;,                      &lt;span class=&quot;comment&quot;&gt;# 初始的学习率增加的策略，linear为线性增加&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    warmup_iters=&lt;span class=&quot;number&quot;&gt;500&lt;/span&gt;,                     &lt;span class=&quot;comment&quot;&gt;# 在初始的500次迭代中学习率逐渐增加&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    warmup_ratio=&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; / &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,                 &lt;span class=&quot;comment&quot;&gt;# 起始的学习率&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    step=[&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;])                         &lt;span class=&quot;comment&quot;&gt;# 在第8和11个epoch时降低学习率&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;checkpoint_config = dict(interval=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)      &lt;span class=&quot;comment&quot;&gt;# 每1个epoch存储一次模型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# yapf:disable&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;log_config = dict(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    interval=&lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;,                          &lt;span class=&quot;comment&quot;&gt;# 每50个batch输出一次信息&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    hooks=[&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        dict(type=&lt;span class=&quot;string&quot;&gt;&#39;TextLoggerHook&#39;&lt;/span&gt;),      &lt;span class=&quot;comment&quot;&gt;# 控制台输出信息的风格&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# dict(type=&#39;TensorboardLoggerHook&#39;)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# yapf:enable&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# runtime settings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;total_epochs = &lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;                               &lt;span class=&quot;comment&quot;&gt;# 最大epoch数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;dist_params = dict(backend=&lt;span class=&quot;string&quot;&gt;&#39;nccl&#39;&lt;/span&gt;)              &lt;span class=&quot;comment&quot;&gt;# 分布式参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;log_level = &lt;span class=&quot;string&quot;&gt;&#39;INFO&#39;&lt;/span&gt;                              &lt;span class=&quot;comment&quot;&gt;# 输出信息的完整度级别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;work_dir = &lt;span class=&quot;string&quot;&gt;&#39;./work_dirs/faster_rcnn_r50_fpn_1x&#39;&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# log文件和模型文件存储路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;load_from = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;                                &lt;span class=&quot;comment&quot;&gt;# 加载模型的路径，None表示从预训练模型加载&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;resume_from = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;                              &lt;span class=&quot;comment&quot;&gt;# 恢复训练模型的路径&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;workflow = [(&lt;span class=&quot;string&quot;&gt;&#39;train&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)]                       &lt;span class=&quot;comment&quot;&gt;# 当前工作区名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;训练函数调用&quot;&gt;&lt;a href=&quot;#训练函数调用&quot; class=&quot;headerlink&quot; title=&quot;训练函数调用&quot;&gt;&lt;/a&gt;训练函数调用&lt;/h1&gt;&lt;h2 id=&quot;训练脚本&quot;&gt;&lt;a href=&quot;#训练脚本&quot; class=&quot;headerlink&quot; title=&quot;训
    
    </summary>
    
      <category term="mmDetection源码分析" scheme="http://chr10003566.github.io./categories/mmDetection%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>FCOS- Fully Convolutional One-Stage Object Detection论文解读</title>
    <link href="http://chr10003566.github.io./2019/12/03/%E5%AD%A6%E4%B9%A0%20FCOS-%20Fully%20Convolutional%20One-Stage%20Object%20Detection/"/>
    <id>http://chr10003566.github.io./2019/12/03/学习 FCOS- Fully Convolutional One-Stage Object Detection/</id>
    <published>2019-12-03T07:24:14.000Z</published>
    <updated>2019-12-03T07:58:35.712Z</updated>
    
    <content type="html">&lt;h1 id=&quot;FCOS-Fully-Convolutional-One-Stage-Object-Detection解读&quot;&gt;&lt;a href=&quot;#FCOS-Fully-Convolutional-One-Stage-Object-Detection解读&quot; class=&quot;headerlink&quot; title=&quot;FCOS- Fully Convolutional One-Stage Object Detection解读&quot;&gt;&lt;/a&gt;FCOS- Fully Convolutional One-Stage Object Detection解读&lt;/h1&gt;&lt;p&gt;本文属于个人基于论文的一些标注与理解，大部分截至原文的内容，未读过论文的同学看起来可能会有点吃力，可以先阅读&lt;a href=&quot;https://blog.csdn.net/u014380165&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AI之路&lt;/a&gt;博主对论文的解读&lt;a href=&quot;https://blog.csdn.net/u014380165/article/details/90962991&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FCOS算法详解&lt;/a&gt;。若已经通读论文的同学，可以阅读本文，或许能为您查缺补漏。谢谢～&lt;/p&gt;
&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;h3 id=&quot;1-Anchor-Based存在的问题&quot;&gt;&lt;a href=&quot;#1-Anchor-Based存在的问题&quot; class=&quot;headerlink&quot; title=&quot;1. Anchor-Based存在的问题&quot;&gt;&lt;/a&gt;1. Anchor-Based存在的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the scales and aspect ratios of anchor boxes are kept fixed, detectors encounter difficulties to deal with object candidates with large shape variations, par- ticularly for small objects.&lt;/p&gt;
&lt;p&gt;anchor-boxes 尺寸和比例固定，导致其处理尺度变化大或者尺寸小的物体有难度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The excessive number of negative samples aggravates the imbalance between positive and negative samples in training.&lt;/p&gt;
&lt;p&gt;基于anchor-based的算法，由于要照顾到recall，要定义较多的尺寸和比例，因此会导致在训练过程，正负样本数量差异较大。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;involve complicated computation such as calculating the intersection-over-union (IoU) scores with ground-truth bounding boxes&lt;/p&gt;
&lt;p&gt;anchor-based算法需要复杂的计算（候选框和真值框的IoU）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-FCNs-have-achieved-tremendous-success-in-dense-prediction-tasks&quot;&gt;&lt;a href=&quot;#2-FCNs-have-achieved-tremendous-success-in-dense-prediction-tasks&quot; class=&quot;headerlink&quot; title=&quot;2. FCNs have achieved tremendous success in dense prediction tasks&quot;&gt;&lt;/a&gt;2. FCNs have achieved tremendous success in dense prediction tasks&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can we solve object detection in the neat per-pixel prediction fashion, analogue to FCN for semantic segmentation, for example&lt;/p&gt;
&lt;p&gt;FCN在分割，关键点检测上取得了不错的效果，能否在目标检测上有所表现。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;3-DenseBox存在的问题&quot;&gt;&lt;a href=&quot;#3-DenseBox存在的问题&quot; class=&quot;headerlink&quot; title=&quot;3. DenseBox存在的问题&quot;&gt;&lt;/a&gt;3. DenseBox存在的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;DenseBox crops and resizes training images to a fixed scale. Thus DenseBox has to perform detection on image pyramids, which is against FCN’s philosophy of computing all convolutions once.&lt;/p&gt;
&lt;p&gt;由于DenseBox需要将候选框裁剪到固定的尺寸，因此其必须使用图像金字塔进行训练。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the highly overlapped bounding boxes result in an intractable ambiguity&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.superbed.cn/item/5de60f58f1f6f81c504fd591.png&quot; alt=&quot;DenseBox&quot;&gt;&lt;/p&gt;
&lt;p&gt;  更重要的原因是，在常规的目标检测中，高度重合的候选框很常见。同一个像素点要回归哪一个候选框是难以界定的。&lt;/p&gt;
&lt;h2 id=&quot;FCOS的优势&quot;&gt;&lt;a href=&quot;#FCOS的优势&quot; class=&quot;headerlink&quot; title=&quot;FCOS的优势&quot;&gt;&lt;/a&gt;FCOS的优势&lt;/h2&gt;&lt;h3 id=&quot;1-proposal-free-and-anchor-free&quot;&gt;&lt;a href=&quot;#1-proposal-free-and-anchor-free&quot; class=&quot;headerlink&quot; title=&quot;1. proposal free and anchor free&quot;&gt;&lt;/a&gt;1. proposal free and anchor free&lt;/h3&gt;&lt;p&gt;减少超参数的个数&lt;/p&gt;
&lt;h3 id=&quot;2-avoids-the-complicated-computation-related-to-anchor-boxes-such-as-the-IOU-computation-and-matching-between-the-anchor-boxes-and-ground-truth-boxes-during-training&quot;&gt;&lt;a href=&quot;#2-avoids-the-complicated-computation-related-to-anchor-boxes-such-as-the-IOU-computation-and-matching-between-the-anchor-boxes-and-ground-truth-boxes-during-training&quot; class=&quot;headerlink&quot; title=&quot;2. avoids the complicated computation related to anchor boxes such as the IOU computation and matching between the anchor boxes and ground-truth boxes during training&quot;&gt;&lt;/a&gt;2. avoids the complicated computation related to anchor boxes such as the IOU computation and matching between the anchor boxes and ground-truth boxes during training&lt;/h3&gt;&lt;p&gt;取消了anchor，就减少了候选框与GT框IoU计算的步骤&lt;/p&gt;
&lt;h3 id=&quot;3-achieve-state-of-the-art-results-among-one-stage-detectors&quot;&gt;&lt;a href=&quot;#3-achieve-state-of-the-art-results-among-one-stage-detectors&quot; class=&quot;headerlink&quot; title=&quot;3. achieve state-of-the- art results among one-stage detectors&quot;&gt;&lt;/a&gt;3. achieve state-of-the- art results among one-stage detectors&lt;/h3&gt;&lt;p&gt;达到了当前one-stage检测器的最好效果&lt;/p&gt;
&lt;p&gt;show that the proposed FCOS can be used as a Region Proposal Networks (RPNs) in two-stage detectors and can achieve significantly better performance than its anchor-based RPN counterpartsFCOS&lt;/p&gt;
&lt;p&gt;甚至可以取代RPNs，用在两阶段的检测器中，并且能达到更好的效果。&lt;/p&gt;
&lt;h3 id=&quot;4-immediately-extended-to-solve-other-vision-tasks-with-minimal-modification-including-instance-segmentation-and-key-point-detection&quot;&gt;&lt;a href=&quot;#4-immediately-extended-to-solve-other-vision-tasks-with-minimal-modification-including-instance-segmentation-and-key-point-detection&quot; class=&quot;headerlink&quot; title=&quot;4. immediately extended to solve other vision tasks with minimal modification, including instance segmentation and key-point detection.&quot;&gt;&lt;/a&gt;4. immediately extended to solve other vision tasks with minimal modification, including instance segmentation and key-point detection.&lt;/h3&gt;&lt;p&gt;很小的改变就可以用于实例分割和关键点检测等领域。&lt;/p&gt;
&lt;h2 id=&quot;FCOS检测器&quot;&gt;&lt;a href=&quot;#FCOS检测器&quot; class=&quot;headerlink&quot; title=&quot;FCOS检测器&quot;&gt;&lt;/a&gt;FCOS检测器&lt;/h2&gt;&lt;h3 id=&quot;1-FCOS模型设计&quot;&gt;&lt;a href=&quot;#1-FCOS模型设计&quot; class=&quot;headerlink&quot; title=&quot;1. FCOS模型设计&quot;&gt;&lt;/a&gt;1. FCOS模型设计&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Different from anchor-based detectors, which consider the location on the input image as the center of (multiple) anchor boxes and regress the target bounding box with these anchor boxes as references, we directly regress the target bounding box at the location&lt;/p&gt;
&lt;p&gt;将Feature map上的点映射回原图后，不使用Anchor而是直接回归目标候选框的位置。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a location falls into multiple bounding boxes, it is considered as an ambiguous sample. We simply choose the bounding box with minimal area as its regression target. &lt;/p&gt;
&lt;p&gt;与DenseBox的思路一致：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若Feature map上的点映射会原图，如果落入GT框中，则认为该点是正样本。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果该位置同时落到了多个GT框中，选择回归到面积更小的候选框中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is worth noting that FCOS can leverage as many fore- ground samples as possible to train the regressor. It is dif- ferent from anchor-based detectors, which only consider the anchor boxes with a highly enough IOU with ground-truth boxes as positive samples. We argue that it may be one of the reasons that FCOS outperforms its anchor-based counterparts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;作者认为因为将Feature map上所有的点作为样本点，落入GT框的点都算正样本，会有更多的正样本加入回归器，而Anchor-based的检测器由于只使用那些与GT框 IOU较大的anchor作为训练样本。这可能是FCOS比anchor-based算法表现更好的原因。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://pic.superbed.cn/item/5de60fadf1f6f81c504fdf0e.png&quot; alt=&quot;model&quot;&gt;&lt;/p&gt;
&lt;p&gt;  从结构上来看，似乎与RetinaNet十分的相似。多了一个center-ness的branch！&lt;/p&gt;
&lt;h3 id=&quot;2-损失函数&quot;&gt;&lt;a href=&quot;#2-损失函数&quot; class=&quot;headerlink&quot; title=&quot;2. 损失函数&quot;&gt;&lt;/a&gt;2. 损失函数&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://pic2.superbed.cn/item/5de60fe6f1f6f81c504fe443.png&quot; alt=&quot;loss&quot;&gt;&lt;/p&gt;
&lt;p&gt;  分类使用Focal loss的分类损失函数，而回归则采用了IoU loss。&lt;/p&gt;
&lt;p&gt;   对feature map中所有点都会计算分类损失，而对正样本的点计算回归损失。&lt;/p&gt;
&lt;h2 id=&quot;FCOS多层级预测&quot;&gt;&lt;a href=&quot;#FCOS多层级预测&quot; class=&quot;headerlink&quot; title=&quot;FCOS多层级预测&quot;&gt;&lt;/a&gt;FCOS多层级预测&lt;/h2&gt;&lt;h3 id=&quot;1-模块解决问题&quot;&gt;&lt;a href=&quot;#1-模块解决问题&quot; class=&quot;headerlink&quot; title=&quot;1. 模块解决问题&quot;&gt;&lt;/a&gt;1. 模块解决问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The large stride of the final feature maps in a CNN can result in a relatively low best possible recall (BPR)&lt;/p&gt;
&lt;p&gt;存在问题：感受野过大，导致小物体漏检，recall会比较低。对于FCOS而言，下降x16后，小物体的点可能也没有了，则会导致recall下降。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Overlaps in ground-truth boxes can cause intractable ambiguity&lt;/p&gt;
&lt;p&gt;真值框重叠的话会导致 训练的时候，特征图上的像素点不知道回归到哪一个真值框。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-we-directly-limit-the-range-of-bounding-box-regression-for-each-level&quot;&gt;&lt;a href=&quot;#2-we-directly-limit-the-range-of-bounding-box-regression-for-each-level&quot; class=&quot;headerlink&quot; title=&quot;2. we directly limit the range of bounding box regression for each level.&quot;&gt;&lt;/a&gt;2. we directly limit the range of bounding box regression for each level.&lt;/h3&gt;&lt;p&gt; we firstly compute the regression targets l∗, t∗, r∗ and b∗for each location on all feature levels. Next, if a location satisfies max(l∗, t∗, r∗, b∗) &amp;gt; mi or max(l∗, t∗, r∗, b∗) &amp;lt; mi−1, it is set as a negative sample and is thus not required to regress a bounding box any- more.&lt;/p&gt;
&lt;p&gt;Since objects with different sizes are assigned to different feature levels and most overlapping happens between ob- jects with considerably different sizes. If a location, even with multi-level prediction used, is still assigned to more than one ground-truth boxes, we simply choose the groundtruth box with minimal area as its target. &lt;/p&gt;
&lt;p&gt;FCOS在FPN每一层的特征图上都做回归，如果回归的得到的结果超过了预定的边界，则认为该像素点为负样本，在计算loss的时候不考虑该点。&lt;/p&gt;
&lt;p&gt; 在解决真值框重叠问题的情况中，作者是这样解释的，不同尺寸的物体是通过FPN的不同层检测出来的，大部分真值框重叠都是真值框大小不同的情况。在同一层特征图中，某一个像素点仍然分配到两个真值框的话，那就回归到那个面积更小的框！&lt;/p&gt;
&lt;h2 id=&quot;3-Center-ness-for-FCOS&quot;&gt;&lt;a href=&quot;#3-Center-ness-for-FCOS&quot; class=&quot;headerlink&quot; title=&quot;3. Center-ness for FCOS&quot;&gt;&lt;/a&gt;3. Center-ness for FCOS&lt;/h2&gt;&lt;h3 id=&quot;due-to-a-lot-of-low-quality-predicted-bounding-boxes-produced-by-locations-far-away-from-the-center-of-an-object&quot;&gt;&lt;a href=&quot;#due-to-a-lot-of-low-quality-predicted-bounding-boxes-produced-by-locations-far-away-from-the-center-of-an-object&quot; class=&quot;headerlink&quot; title=&quot;due to a lot of low-quality predicted bounding boxes produced by locations far away from the center of an object.&quot;&gt;&lt;/a&gt;due to a lot of low-quality predicted bounding boxes produced by locations far away from the center of an object.&lt;/h3&gt;&lt;p&gt;存在很多低质量，远离物体中心点的矩形框&lt;/p&gt;
&lt;h3 id=&quot;The-center-ness-depicts-the-normalized-distance-from-the-location-to-the-center-of-the-object&quot;&gt;&lt;a href=&quot;#The-center-ness-depicts-the-normalized-distance-from-the-location-to-the-center-of-the-object&quot; class=&quot;headerlink&quot; title=&quot;The center-ness depicts the normalized distance from the location to the center of the object&quot;&gt;&lt;/a&gt;The center-ness depicts the normalized distance from the location to the center of the object&lt;/h3&gt;&lt;h3 id=&quot;The-center-ness-ranges-from-0-to-1-and-is-thus-trained-with-binary-cross-entropy-BCE-loss-The-loss-is-added-to-the-loss-function-Eq-2-When-testing-the-final-score-used-for-ranking-the-detected-bounding-boxes-is-computed-by-multiplying-the-predicted-center-ness-with-the-correspond-ing-classification-score-Thus-the-center-ness-can-down-weight-the-scores-of-bounding-boxes-far-from-the-center-of-an-object&quot;&gt;&lt;a href=&quot;#The-center-ness-ranges-from-0-to-1-and-is-thus-trained-with-binary-cross-entropy-BCE-loss-The-loss-is-added-to-the-loss-function-Eq-2-When-testing-the-final-score-used-for-ranking-the-detected-bounding-boxes-is-computed-by-multiplying-the-predicted-center-ness-with-the-correspond-ing-classification-score-Thus-the-center-ness-can-down-weight-the-scores-of-bounding-boxes-far-from-the-center-of-an-object&quot; class=&quot;headerlink&quot; title=&quot;The center-ness ranges from 0 to 1 and is thus trained with binary cross entropy (BCE) loss. The loss is added to the loss function Eq. (2). When testing, the final score (used for ranking the detected bounding boxes) is computed by multiplying the predicted center-ness with the correspond- ing classification score. Thus the center-ness can down- weight the scores of bounding boxes far from the center of an object.&quot;&gt;&lt;/a&gt;The center-ness ranges from 0 to 1 and is thus trained with binary cross entropy (BCE) loss. The loss is added to the loss function Eq. (2). When testing, the final score (used for ranking the detected bounding boxes) is computed by multiplying the predicted center-ness with the correspond- ing classification score. Thus the center-ness can down- weight the scores of bounding boxes far from the center of an object.&lt;/h3&gt;&lt;p&gt;在训练阶段：center-ness用交叉熵损失函数，测试阶段：center-ness x classification score作为最后classification score的值，通过这种方式将远离中心点的候选框的置信度降低。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;FCOS-Fully-Convolutional-One-Stage-Object-Detection解读&quot;&gt;&lt;a href=&quot;#FCOS-Fully-Convolutional-One-Stage-Object-Detection解读&quot; class=&quot;heade
    
    </summary>
    
      <category term="论文阅读" scheme="http://chr10003566.github.io./categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>mmDetection源码分析（一）：inference阶段代码调用</title>
    <link href="http://chr10003566.github.io./2019/12/02/mmdetection(1)/"/>
    <id>http://chr10003566.github.io./2019/12/02/mmdetection(1)/</id>
    <published>2019-12-02T08:48:14.000Z</published>
    <updated>2019-12-02T09:57:04.368Z</updated>
    
    <content type="html">&lt;h1 id=&quot;mmdetection介绍&quot;&gt;&lt;a href=&quot;#mmdetection介绍&quot; class=&quot;headerlink&quot; title=&quot;mmdetection介绍&quot;&gt;&lt;/a&gt;mmdetection介绍&lt;/h1&gt;&lt;p&gt;mmDetection（&lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;mmdetection&lt;/a&gt;）应该是目前最流行的检测网络框架了，由香港中文大学与商汤合作维护的一个检测工具箱，目前仍然在不断的更新中。&lt;/p&gt;
&lt;p&gt;本系列打算研究学习下如何使用mmDetection去复现其他论文的检测网络模型或者构建自己的网络模型，以及如何使用mmDetection训练自己的数据。网上已有不少mmDetection源码分析博客，大家也都是按照各自学习的节奏来排版的，这里记录下学习的过程。&lt;/p&gt;
&lt;p&gt;照例将学习的链接贴出，尊重原创～&lt;/p&gt;
&lt;p&gt;DateLoader学习链接：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/u014380165/article/details/79058479&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AI之路-PyTorch源码解读之torch.utils.data.DataLoader&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/76893455&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/TH_NUM/article/details/80877687&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;pytorch 函数DataLoade&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;推理阶段—demo-py（通过读取一张图像，显示效果）&quot;&gt;&lt;a href=&quot;#推理阶段—demo-py（通过读取一张图像，显示效果）&quot; class=&quot;headerlink&quot; title=&quot;推理阶段—demo.py（通过读取一张图像，显示效果）&quot;&gt;&lt;/a&gt;推理阶段—&lt;code&gt;demo.py&lt;/code&gt;（通过读取一张图像，显示效果）&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; mmdet.apis &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; init_detector, inference_detector, show_result&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; mmcv&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;config_file = &lt;span class=&quot;string&quot;&gt;&#39;configs/faster_rcnn_r50_fpn_1x.py&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;checkpoint_file = &lt;span class=&quot;string&quot;&gt;&#39;checkpoints/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# build the model from a config file and a checkpoint file&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;model = init_detector(config_file, checkpoint_file, device=&lt;span class=&quot;string&quot;&gt;&#39;cuda:0&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# test a single image and show the results&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;img = &lt;span class=&quot;string&quot;&gt;&#39;test.jpg&#39;&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# or img = mmcv.imread(img), which will only load it once&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result = inference_detector(model, img)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# visualize the results in a new window&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;show_result(img, result, model.CLASSES)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# or save the visualization results to image files&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;show_result(img, result, model.CLASSES, out_file=&lt;span class=&quot;string&quot;&gt;&#39;result.jpg&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;init-detector&quot;&gt;&lt;a href=&quot;#init-detector&quot; class=&quot;headerlink&quot; title=&quot;init_detector&quot;&gt;&lt;/a&gt;&lt;code&gt;init_detector&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;init_detector三个输入参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;config：检测模型的配置文件，一般位于 mmdetection/config/ 中&lt;/li&gt;
&lt;li&gt;checkpoint：即训练好的权重 在mmdetection的model zoo中可以下载&lt;/li&gt;
&lt;li&gt;device：分配到的设备对象&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;返回：model（检测模型）&lt;/p&gt;
&lt;h2 id=&quot;inference-detector&quot;&gt;&lt;a href=&quot;#inference-detector&quot; class=&quot;headerlink&quot; title=&quot;inference_detector&quot;&gt;&lt;/a&gt;&lt;code&gt;inference_detector&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;inference_detector 输入两个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model: init_detector返回的model&lt;/li&gt;
&lt;li&gt;imgs: 图像的路径&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;返回：测试图像的结果&lt;/p&gt;
&lt;h3 id=&quot;inference-single&quot;&gt;&lt;a href=&quot;#inference-single&quot; class=&quot;headerlink&quot; title=&quot;_ inference_single&quot;&gt;&lt;/a&gt;&lt;code&gt;_ inference_single&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;将输入的图像进过resize后，输入到模型中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;_prepare_data&lt;/p&gt;
&lt;p&gt;用于将图像转换成config中test.img_scale&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;inference-generator&quot;&gt;&lt;a href=&quot;#inference-generator&quot; class=&quot;headerlink&quot; title=&quot;_ inference_generator&quot;&gt;&lt;/a&gt;&lt;code&gt;_ inference_generator&lt;/code&gt;&lt;/h3&gt;&lt;h2 id=&quot;show-result&quot;&gt;&lt;a href=&quot;#show-result&quot; class=&quot;headerlink&quot; title=&quot;show_result&quot;&gt;&lt;/a&gt;&lt;code&gt;show_result&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;show_result参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;img :图像的路径（字符串）&lt;/li&gt;
&lt;li&gt;result：inference_detector的返回值&lt;/li&gt;
&lt;li&gt;class_name：训练图像的类别名称&lt;br&gt;阈值&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;推理阶段—-tools-test-py（对数据集进行测试）&quot;&gt;&lt;a href=&quot;#推理阶段—-tools-test-py（对数据集进行测试）&quot; class=&quot;headerlink&quot; title=&quot;推理阶段—./tools/test.py（对数据集进行测试）&quot;&gt;&lt;/a&gt;推理阶段—&lt;code&gt;./tools/test.py&lt;/code&gt;（对数据集进行测试）&lt;/h1&gt;&lt;p&gt;通过&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# single-gpu testing&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;python tools/test.py $&amp;#123;CONFIG_FILE&amp;#125; $&amp;#123;CHECKPOINT_FILE&amp;#125; [--out $&amp;#123;RESULT_FILE&amp;#125;] [--eval $&amp;#123;EVAL_METRICS&amp;#125;] [--show]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# multi-gpu testing&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;./tools/dist_test.sh $&amp;#123;CONFIG_FILE&amp;#125; $&amp;#123;CHECKPOINT_FILE&amp;#125; $&amp;#123;GPU_NUM&amp;#125; [--out $&amp;#123;RESULT_FILE&amp;#125;] [--eval $&amp;#123;EVAL_METRICS&amp;#125;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;build-dataset&quot;&gt;&lt;a href=&quot;#build-dataset&quot; class=&quot;headerlink&quot; title=&quot;build_dataset&quot;&gt;&lt;/a&gt;&lt;code&gt;build_dataset&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;build_dataset一个输入参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cfg.data.test&lt;/code&gt;: 是mmcv.utils.config.ConfigDict类型。cfg则是通过&lt;code&gt;mmcv.Config.fromfile&lt;/code&gt;(mmdetection/config/xx.py的路径）转换成&lt;code&gt;mmcv.utils.config.Config&lt;/code&gt;得到。&lt;code&gt;cfg.data.test&lt;/code&gt;：包含了测试集的路径，标注文件路径，测试图像大小等信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;返回：mmdet定义的Dataset类（是&lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;的子类）&lt;/p&gt;
&lt;h2 id=&quot;build-dataloader&quot;&gt;&lt;a href=&quot;#build-dataloader&quot; class=&quot;headerlink&quot; title=&quot;build_dataloader&quot;&gt;&lt;/a&gt;&lt;code&gt;build_dataloader&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;build_dataloader:将数据转换成pytorch可读的类型&lt;/p&gt;
&lt;p&gt;DataLoader的学习链接见上～&lt;/p&gt;
&lt;h2 id=&quot;build-detector&quot;&gt;&lt;a href=&quot;#build-detector&quot; class=&quot;headerlink&quot; title=&quot;build_detector&quot;&gt;&lt;/a&gt;&lt;code&gt;build_detector&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;build_detector三个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cfg: 仍然是目标检测框架配置文件（如：faster_rcnn_r50_fpn_1x.py）中model部分。需要包括目标检测框架backbone、neck、head等部分信息&lt;/li&gt;
&lt;li&gt;train_cfg: 推理阶段，该参数为None&lt;/li&gt;
&lt;li&gt;test_cfg: cfg中test_cfg字段的内容，需要包含阈值等信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;init_detector中包含函数，init_detector=build_detector + load_checkpoint&lt;/p&gt;
&lt;h2 id=&quot;load-checkpoint&quot;&gt;&lt;a href=&quot;#load-checkpoint&quot; class=&quot;headerlink&quot; title=&quot;load_checkpoint&quot;&gt;&lt;/a&gt;&lt;code&gt;load_checkpoint&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;load_checkpoint两个个主要输入参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model：build_detecor返回的model&lt;/li&gt;
&lt;li&gt;checkpoint：训练权重（.pth，如 /models/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;gpt-test&quot;&gt;&lt;a href=&quot;#gpt-test&quot; class=&quot;headerlink&quot; title=&quot;gpt_test&quot;&gt;&lt;/a&gt;gpt_test&lt;/h2&gt;&lt;h3 id=&quot;single-gpu-test&quot;&gt;&lt;a href=&quot;#single-gpu-test&quot; class=&quot;headerlink&quot; title=&quot;single_gpu_test&quot;&gt;&lt;/a&gt;&lt;code&gt;single_gpu_test&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;single_gpu_test三个输入参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model：build_detector返回的model，并通过 load_checkpoint初始化了权重。&lt;/li&gt;
&lt;li&gt;data_loader：build_dataloader返回的DataLoader&lt;/li&gt;
&lt;li&gt;show：默认为False&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;multi-gpu-test&quot;&gt;&lt;a href=&quot;#multi-gpu-test&quot; class=&quot;headerlink&quot; title=&quot;multi_gpu_test&quot;&gt;&lt;/a&gt;&lt;code&gt;multi_gpu_test&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;与single_gpu_test相似，用多块GPU进行测试。&lt;/p&gt;
&lt;h2 id=&quot;eval&quot;&gt;&lt;a href=&quot;#eval&quot; class=&quot;headerlink&quot; title=&quot;eval&quot;&gt;&lt;/a&gt;eval&lt;/h2&gt;&lt;p&gt;调用&lt;code&gt;mmdet.core&lt;/code&gt;中&lt;code&gt;coco_eval&lt;/code&gt;，根据&lt;code&gt;eval_type&lt;/code&gt;对COCO数据集进行评估。&lt;/p&gt;
&lt;p&gt;eval_type有以下几种：&lt;br&gt;&lt;code&gt;&amp;#39;proposal&amp;#39;, &amp;#39;proposal_fast&amp;#39;, &amp;#39;bbox&amp;#39;, &amp;#39;segm&amp;#39;, &amp;#39;keypoints&amp;#39;&lt;/code&gt;.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;mmdetection介绍&quot;&gt;&lt;a href=&quot;#mmdetection介绍&quot; class=&quot;headerlink&quot; title=&quot;mmdetection介绍&quot;&gt;&lt;/a&gt;mmdetection介绍&lt;/h1&gt;&lt;p&gt;mmDetection（&lt;a href=&quot;https
    
    </summary>
    
      <category term="mmDetection源码分析" scheme="http://chr10003566.github.io./categories/mmDetection%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>2D坐标系与3D坐标系的相互转换--python实现</title>
    <link href="http://chr10003566.github.io./2019/07/15/2D%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E3D%E5%9D%90%E6%A0%87%E7%B3%BB%E7%9A%84%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2--python%E5%AE%9E%E7%8E%B0/"/>
    <id>http://chr10003566.github.io./2019/07/15/2D坐标系与3D坐标系的相互转换--python实现/</id>
    <published>2019-07-15T05:48:14.000Z</published>
    <updated>2019-07-15T09:22:14.999Z</updated>
    
    <content type="html">&lt;p&gt;并不是做关于SLAM方向的，但由于某些任务涉及到，故作此笔记～&lt;/p&gt;
&lt;p&gt;相机内参矩阵：&lt;br&gt;不同的的深度摄像头具有不同的特征参数，在计算机视觉里，将这组参数设置为相机的内参矩阵C:&lt;br&gt;$$\begin{bmatrix} f_x&amp;amp; 0 &amp;amp;c_x \\ 0 &amp;amp; f_y &amp;amp; c_y \\ 0 &amp;amp; 0 &amp;amp; 1  \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;  fx，fy指相机在x轴和y轴上的焦距，cx，cy是相机的光圈中心，这组参数是摄像头生产制作之后就固定的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;世界坐标系:&lt;/strong&gt; 用户定义的三维师姐坐标系，以某个点为远点，为描述目标物在真实师姐里的位置而被引入&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;相机坐标系:&lt;/strong&gt; 以相机为原点建立的坐标系，为了从相机的角度描述物体的位置而定义，作为沟通世界坐标系和图像/像素坐标系的中间一环。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习自:&lt;/strong&gt; &lt;a href=&quot;https://blog.csdn.net/mightbxg/article/details/79363699&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;三维坐标变换——旋转矩阵与旋转向量&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20161210141917167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;旋转矩阵:&lt;/strong&gt;&lt;br&gt;  &lt;img src=&quot;https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvSFRCMTljZTVha1AyZ0swalNaUHhxNnljUXBYYWQuanBn&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;使用矩阵来表示一个旋转关系有两个缺点： 1）通过旋转矩阵不能直观地看出旋转的方向和角度 2）另一方面：旋转变换有3个自由度，旋转矩阵中的元素不是相互独立的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;旋转向量:&lt;/strong&gt;&lt;br&gt;设旋转向量的单位向量为r，模为θ。三维点（或者说三维向量）p在旋转向量 r 的作用下变换至 p′，则：&lt;br&gt;$$p’=\cos\theta\cdot p+(1-\cos\theta)(p\cdot r)r+\sin\theta\cdot r \times$$ p&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习自:&lt;/strong&gt;&lt;a href=&quot;https://blog.csdn.net/chentravelling/article/details/53558096&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;计算机视觉：相机成像原理：世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvSFRCMXo0aTZha1AyZ0swalNaUHhxNnljUXBYYVcuanBn&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从世界坐标系到相机坐标系:&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20161210142437060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;相机坐标系与图像坐标系:&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20161210142740999?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图像坐标系与像素坐标系:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;像素坐标系和图像坐标系都在成像平面，只是各自的原点和度量单位不大一样，图像坐标系的原点为相机光轴与成像平面的交点。图像坐标系的单位是mm，属于物理单位，而像素坐标系的单位是pixel&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20161210143514044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;像素坐标系与世界坐标系:&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20161210144703071?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot;&gt; &lt;/p&gt;
&lt;h2 id=&quot;convert2Dto3D&quot;&gt;&lt;a href=&quot;#convert2Dto3D&quot; class=&quot;headerlink&quot; title=&quot;convert2Dto3D&quot;&gt;&lt;/a&gt;convert2Dto3D&lt;/h2&gt;&lt;p&gt;通过上面的公式，将像素坐标系的点映射到世界坐标系上，由于二维坐标映射到三维坐标会多了一个深度信息，因此二维点映射到三维坐标系，得到的是一条射线。虽然通过下面的代码得到的确实是一个三维的坐标点，但是其实是不确定的，与光点连线上（射线）所有的点都满足条件&lt;/p&gt;
&lt;h2 id=&quot;convert3Dto2D&quot;&gt;&lt;a href=&quot;#convert3Dto2D&quot; class=&quot;headerlink&quot; title=&quot;convert3Dto2D&quot;&gt;&lt;/a&gt;convert3Dto2D&lt;/h2&gt;&lt;p&gt;将3D平面的box转成2D平面的box，通过cv.projectPoint(三维平面下的8个点，旋转矩阵， 平移向量， 相机内参， 0)可以得到3D平面下的8个点映射到2维平面下8个点的坐标&lt;/p&gt;
&lt;h2 id=&quot;代码&quot;&gt;&lt;a href=&quot;#代码&quot; class=&quot;headerlink&quot; title=&quot;代码&quot;&gt;&lt;/a&gt;代码&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;85&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 此函数只是外部定义而已，大家可自行定义&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;camera_matrix, rvec, tvec = camera_params()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;相机内参:&quot;&lt;/span&gt;, camera_matrix)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;平移向量:&quot;&lt;/span&gt;, tvec)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;旋转矩阵:&quot;&lt;/span&gt;, rvec)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# (R T, 0 1)矩阵&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Trans = np.hstack((rvec, [[tvec[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]], [tvec[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]], [tvec[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]]]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 相机内参和相机外参 矩阵相乘&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;temp = np.dot(camera_matrix, Trans)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Pp = np.linalg.pinv(temp)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 点（u, v, 1) 对应代码里的 [605,341,1]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;p1 = np.array([&lt;span class=&quot;number&quot;&gt;605&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;341&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], np.float)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;像素坐标系的点:&quot;&lt;/span&gt;, p1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X = np.dot(Pp, p1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;X:&quot;&lt;/span&gt;, X)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 与Zc相除 得到世界坐标系的某一个点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X1 = np.array(X[:&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], np.float)/X[&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;X1:&quot;&lt;/span&gt;, X1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## 3D 转成 2D&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; utils &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; cv2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## 2D 转成 3D&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 此函数只是外部定义而已，大家可自行定义&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;camera_matrix, rvec, tvec = camera_params()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;相机内参:&quot;&lt;/span&gt;, camera_matrix)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;平移向量:&quot;&lt;/span&gt;, tvec)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;旋转矩阵:&quot;&lt;/span&gt;, rvec)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# (R T, 0 1)矩阵&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Trans = np.hstack((rvec, [[tvec[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]], [tvec[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]], [tvec[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]]]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 相机内参和相机外参 矩阵相乘&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;temp = np.dot(camera_matrix, Trans)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Pp = np.linalg.pinv(temp)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 点（u, v, 1) 对应代码里的 [605,341,1]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;p1 = np.array([&lt;span class=&quot;number&quot;&gt;605&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;341&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], np.float)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;像素坐标系的点:&quot;&lt;/span&gt;, p1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X = np.dot(Pp, p1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;X:&quot;&lt;/span&gt;, X)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 与Zc相除 得到世界坐标系的某一个点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X1 = np.array(X[:&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], np.float)/X[&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;X1:&quot;&lt;/span&gt;, X1)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## 3D 转成 2D&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;## cube为世界坐标系的8个点的三维坐标&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cube = np.float64([[&lt;span class=&quot;number&quot;&gt;-3.102&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;-1.58400011&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9.29399872&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;-3.102&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-0.08400005&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9.29399872&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-1.27200007&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;-0.08400005&lt;/span&gt; , &lt;span class=&quot;number&quot;&gt;9.29399872&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-1.27200007&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1.58400011&lt;/span&gt;  ,&lt;span class=&quot;number&quot;&gt;9.29399872&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-3.102&lt;/span&gt;   ,   &lt;span class=&quot;number&quot;&gt;-1.58400011&lt;/span&gt; ,&lt;span class=&quot;number&quot;&gt;13.8939991&lt;/span&gt; ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-3.102&lt;/span&gt;   ,   &lt;span class=&quot;number&quot;&gt;-0.08400005&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;13.8939991&lt;/span&gt; ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-1.27200007&lt;/span&gt; ,&lt;span class=&quot;number&quot;&gt;-0.08400005&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;13.8939991&lt;/span&gt; ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; ,[&lt;span class=&quot;number&quot;&gt;-1.27200007&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1.58400011&lt;/span&gt; ,&lt;span class=&quot;number&quot;&gt;13.8939991&lt;/span&gt; ]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result, _ = cv2.projectPoints(cube, rvec, tvec, camera_matrix, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;3D to 2D 的 8个点的坐标：&quot;&lt;/span&gt;, result)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;并不是做关于SLAM方向的，但由于某些任务涉及到，故作此笔记～&lt;/p&gt;
&lt;p&gt;相机内参矩阵：&lt;br&gt;不同的的深度摄像头具有不同的特征参数，在计算机视觉里，将这组参数设置为相机的内参矩阵C:&lt;br&gt;$$\begin{bmatrix} f_x&amp;amp; 0 &amp;amp;c_x \
    
    </summary>
    
      <category term="work" scheme="http://chr10003566.github.io./categories/work/"/>
    
    
  </entry>
  
  <entry>
    <title>Java中的多线程实现方式</title>
    <link href="http://chr10003566.github.io./2017/03/03/Java%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/"/>
    <id>http://chr10003566.github.io./2017/03/03/Java中的多线程实现方式/</id>
    <published>2017-03-03T03:31:44.000Z</published>
    <updated>2017-03-03T04:05:10.000Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Java中的多线程实现方式&quot;&gt;&lt;a href=&quot;#Java中的多线程实现方式&quot; class=&quot;headerlink&quot; title=&quot;Java中的多线程实现方式&quot;&gt;&lt;/a&gt;Java中的多线程实现方式&lt;/h1&gt;&lt;p&gt;在我们的开发的过程中，常常会碰到多线程的问题，对于多线程的实现方式主要有两种：&lt;strong&gt;实现Runnable接口&lt;/strong&gt;、&lt;strong&gt;集成Thread类&lt;/strong&gt;。对于这两种多线程实现的方式也是有一些差异的。网上针对此问题基本都是使用&lt;strong&gt;买票系统&lt;/strong&gt;的例子，接下来我们就用代码来模拟下售票系统，实现2个售票点发售10张车票，一个售票点表示一个线程。&lt;/p&gt;
&lt;h2 id=&quot;方案一&quot;&gt;&lt;a href=&quot;#方案一&quot; class=&quot;headerlink&quot; title=&quot;方案一&quot;&gt;&lt;/a&gt;方案一&lt;/h2&gt;&lt;p&gt;首先从最简单的做法开始，开两个Thread类进行售票。&lt;br&gt;&lt;strong&gt;测试代码如下&lt;/strong&gt;：&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ticketThread&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Thread&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ticket = &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;; i++)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(ticket &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					sleep(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					System.out.println(Thread.currentThread().getName() &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;							+ &lt;span class=&quot;string&quot;&gt;&quot;卖票 ——&amp;gt;&quot;&lt;/span&gt; + (ticket--) );					&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&amp;#125; &lt;span class=&quot;keyword&quot;&gt;catch&lt;/span&gt; (Exception e) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&lt;span class=&quot;comment&quot;&gt;// &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; handle exception&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					e.printStackTrace();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;comment&quot;&gt;// TODO Auto-generated method stub&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ticketThread().start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ticketThread().start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试结果：&lt;/strong&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;10&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;10&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;9&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;9&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;8&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;8&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;7&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;7&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;6&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;6&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;5&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;5&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;4&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;4&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;3&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;3&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;2&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;2&lt;br&gt;&lt;br&gt;Thread-0卖票 ——&amp;gt;1&lt;br&gt;&lt;br&gt;Thread-1卖票 ——&amp;gt;1&lt;br&gt;&lt;br&gt;&lt;strong&gt;结论：&lt;/strong&gt;&lt;br&gt;从上面的测试结果可以看出，两个线程各自卖了各自的10张票而不是去卖共同的10张票，这和我们的目标多个线程去处理同一个资源相差很多。我们创建了2个&lt;code&gt;ticketThread&lt;/code&gt;对象就等于创建了2个资源，每个资源有10张票，每个资源都在独自处理各自的资源。所以通过这个例子我们知道，在这个售票系统中，我们只能创建一个资源对象，但需要创建多个线程去处理同一个资源对象，并且每个线程上所运行的是相同的程序代码。&lt;/p&gt;
&lt;h2 id=&quot;方案二&quot;&gt;&lt;a href=&quot;#方案二&quot; class=&quot;headerlink&quot; title=&quot;方案二&quot;&gt;&lt;/a&gt;方案二&lt;/h2&gt;&lt;p&gt;既然只能创建一个资源对象，那么我就只创建一个&lt;code&gt;ticketThread&lt;/code&gt;，在额外创建两个新的线程去实现售票。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试代码：&lt;/strong&gt;&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ticketThread&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Thread&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ticket = &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;; i++)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&lt;span class=&quot;keyword&quot;&gt;synchronized&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(ticket &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						sleep(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						System.out.println(Thread.currentThread().getName() &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;								+ &lt;span class=&quot;string&quot;&gt;&quot;卖票 ——&amp;gt;&quot;&lt;/span&gt; + (&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;.ticket--) );					&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&amp;#125; &lt;span class=&quot;keyword&quot;&gt;catch&lt;/span&gt; (Exception e) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						&lt;span class=&quot;comment&quot;&gt;// &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; handle exception&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						e.printStackTrace();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;comment&quot;&gt;// TODO Auto-generated method stub&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		ticketThread t1 = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ticketThread();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(t1, &lt;span class=&quot;string&quot;&gt;&quot;线程1&quot;&lt;/span&gt;).start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(t1, &lt;span class=&quot;string&quot;&gt;&quot;线程2&quot;&lt;/span&gt;).start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试结果：&lt;/strong&gt;&lt;br&gt;线程1卖票 ——&amp;gt;10&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;9&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;8&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;7&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;6&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;5&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;4&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;3&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;2&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;1&lt;br&gt;&lt;br&gt;&lt;strong&gt;结论：&lt;/strong&gt;&lt;br&gt;这种情况下，我们实现了多个线程对同一个资源进行处理。这里我们创建新线程使用了&lt;br&gt;&lt;code&gt;Thread(ThreadGroup groupOb, String threadName)&lt;/code&gt;&lt;br&gt;把&lt;code&gt;ticketThread&lt;/code&gt;作为参数传入新创建的线程。在这种情况下新创建的两个线程就去执行了&lt;code&gt;ticketThread&lt;/code&gt;中的&lt;code&gt;run()&lt;/code&gt;方法，这样一来就实现了两个线程对同一个资源进行处理。但就原理上来说，此方案与方案三是一样的。&lt;/p&gt;
&lt;h2 id=&quot;方案三&quot;&gt;&lt;a href=&quot;#方案三&quot; class=&quot;headerlink&quot; title=&quot;方案三&quot;&gt;&lt;/a&gt;方案三&lt;/h2&gt;&lt;p&gt;使用&lt;code&gt;Runnable&lt;/code&gt;来实现多线程。&lt;br&gt;&lt;strong&gt;测试代码：&lt;/strong&gt;&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ticketThread&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Runnable&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ticket = &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;; i++)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//			synchronized(this)&amp;#123;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(ticket &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						Thread.sleep(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						System.out.println(Thread.currentThread().getName() &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;								+ &lt;span class=&quot;string&quot;&gt;&quot;卖票 ——&amp;gt;&quot;&lt;/span&gt; + (ticket--) );	 &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&amp;#125; &lt;span class=&quot;keyword&quot;&gt;catch&lt;/span&gt; (Exception e) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						&lt;span class=&quot;comment&quot;&gt;// &lt;span class=&quot;doctag&quot;&gt;TODO:&lt;/span&gt; handle exception&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;						e.printStackTrace();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;					&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;				&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;			&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//		&amp;#125;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;comment&quot;&gt;// TODO Auto-generated method stub&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		ticketThread t1 = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ticketThread();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(t1, &lt;span class=&quot;string&quot;&gt;&quot;线程1&quot;&lt;/span&gt;).start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Thread(t1, &lt;span class=&quot;string&quot;&gt;&quot;线程2&quot;&lt;/span&gt;).start();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试结果：&lt;/strong&gt;&lt;br&gt;线程1卖票 ——&amp;gt;10&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;9&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;7&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;8&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;6&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;5&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;4&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;3&lt;br&gt;&lt;br&gt;线程2卖票 ——&amp;gt;2&lt;br&gt;&lt;br&gt;线程1卖票 ——&amp;gt;1&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;br&gt;在上面的测试代码中，我们创建了2个线程，每个县城调用的是同一个&lt;code&gt;ticketThread&lt;/code&gt;对象中的&lt;code&gt;run()&lt;/code&gt;方法，访问的是同一个对象的变量&lt;code&gt;(ticket)&lt;/code&gt;的实例，这个程序就完美的满足了我们的需求。&lt;/p&gt;
&lt;h3 id=&quot;Runnable与Thread的区别与联系&quot;&gt;&lt;a href=&quot;#Runnable与Thread的区别与联系&quot; class=&quot;headerlink&quot; title=&quot;Runnable与Thread的区别与联系&quot;&gt;&lt;/a&gt;Runnable与Thread的区别与联系&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;一个类只能继承一个父类，这是使用&lt;code&gt;Thread&lt;/code&gt;的局限性，而&lt;code&gt;Runnable&lt;/code&gt;是一个接口，只要实现这个接口就行了。所以在实际的开发过程中，是通过&lt;code&gt;Runnable&lt;/code&gt;接口来实现的，并且&lt;code&gt;Runnable&lt;/code&gt;更适合资源共享的实现。&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;Runnable&lt;/code&gt;可以比喵由于Java的单继承特性带来的局限性，我们经常碰到这样一种情况，即当我们要已经继承了某一个累的子类放入多线程中，由于一个类不能同时偶两个父类，所以不能用继承Thread类的方式，那么这个类就是只能采用Runnable接口的方式了。&lt;/li&gt;
&lt;li&gt;实际上&lt;code&gt;Thread类&lt;/code&gt;也是&lt;code&gt;Runnbale&lt;/code&gt;接口的子类。&lt;br&gt;&lt;code&gt;public class Thread extends Object implements Runnable&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;Runnable&lt;/code&gt;对象时，&lt;code&gt;Runnable&lt;/code&gt;定义的子类没有&lt;code&gt;start()&lt;/code&gt;方法，只有&lt;code&gt;Thread&lt;/code&gt;类中才有，观察&lt;code&gt;Thread&lt;/code&gt;类，有一个构造方法&lt;code&gt;public Thread(Runnable target)&lt;/code&gt;，此构造方法接受&lt;code&gt;Runanble&lt;/code&gt;的子类实例，也就是说可以通过&lt;code&gt;Thread&lt;/code&gt;类来启动&lt;code&gt;Runnable&lt;/code&gt;实现多线程。&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Java中的多线程实现方式&quot;&gt;&lt;a href=&quot;#Java中的多线程实现方式&quot; class=&quot;headerlink&quot; title=&quot;Java中的多线程实现方式&quot;&gt;&lt;/a&gt;Java中的多线程实现方式&lt;/h1&gt;&lt;p&gt;在我们的开发的过程中，常常会碰到多线程的问题，对于多
    
    </summary>
    
      <category term="Java开发学习" scheme="http://chr10003566.github.io./categories/Java%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Android开发学习笔记秘籍(二十一)</title>
    <link href="http://chr10003566.github.io./2017/01/04/tips/"/>
    <id>http://chr10003566.github.io./2017/01/04/tips/</id>
    <published>2017-01-04T03:31:44.000Z</published>
    <updated>2017-12-09T13:23:56.575Z</updated>
    
    <content type="html">&lt;h1 id=&quot;Android开发学习笔记秘籍-二十一&quot;&gt;&lt;a href=&quot;#Android开发学习笔记秘籍-二十一&quot; class=&quot;headerlink&quot; title=&quot;Android开发学习笔记秘籍(二十一)&quot;&gt;&lt;/a&gt;Android开发学习笔记秘籍(二十一)&lt;/h1&gt;&lt;p&gt;时隔半年重新捡起Android开发，以前学习的各种零零散散的Android开发技巧也忘的差不多了，但人总不能走回头路吧，只能猛向前走，遗忘的呢也正好通过翻阅以前的笔记进行复习。&lt;/p&gt;
&lt;p&gt;以前都是学习一些UI方面的知识，很少涉及网络。现在开始往网络方向学习。&lt;/p&gt;
&lt;p&gt;首先是最简单的一些Android小知识点，网上都有。大都一样，我也不知道哪个是原创就不贴链接了。纯粹为了以后自己好用！&lt;/p&gt;
&lt;h2 id=&quot;Android小知识点：&quot;&gt;&lt;a href=&quot;#Android小知识点：&quot; class=&quot;headerlink&quot; title=&quot;Android小知识点：&quot;&gt;&lt;/a&gt;Android小知识点：&lt;/h2&gt;&lt;p&gt;检查Android手机网络是否可用：&lt;/p&gt;
&lt;p&gt;1.获取ConnectivityManager对象&lt;br&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;    Context context = activity.getApplicationContext();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 获取手机所有连接管理对象（包括对wi-fi,net等连接的管理）&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ConnectivityManager connectivityManager = (ConnectivityManager)context.getSystemService(Context.CONNECTIVITY_SERVICE);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.获取NetworkInfo对象&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;NetworkInfo[] networkInfo = connectivityManager.getAllNetworkInfo();&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;3.判断当前网络状态是否未连接在状态&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;if (networkInfo[i].getState() == NetworkInfo.State.CONNECTED)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      return true;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;另外还需要在&lt;code&gt;AndroidManifest.xml&lt;/code&gt;当中添加访问当前网络状态权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;uses-permission android:name=&amp;quot;android.permission.ACCESS_NETWORK_STATE&amp;quot;&amp;gt;&amp;lt;/uses-permission&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;接下来就是重头戏了—-AsyncHttpClient开源库&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://my.oschina.net/u/725054/blog/494494&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;学习链接&lt;/a&gt;：&lt;a href=&quot;https://my.oschina.net/u/725054/blog/494494&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://my.oschina.net/u/725054/blog/494494&lt;/a&gt;&lt;br&gt;这个是一年前的一个在Apache的HttpCLient库的基础上开发构建而成的。现在还常不常用我就不清楚了，不过学习下总是对自己有帮助的。&lt;/p&gt;
&lt;p&gt;网上关于&lt;code&gt;AsyncHttpClient&lt;/code&gt;的介绍呢，统统都是来自官网的翻译—&lt;a href=&quot;http://loopj.com/android-async-http/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://loopj.com/android-async-http/&lt;/a&gt;&lt;br&gt;&lt;br&gt; github—&lt;a href=&quot;https://github.com/loopj/android-async-http&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/loopj/android-async-http&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面的学习链接是个人认为翻译的比较好的，但光有方法使用，没有实例可不行，于是乎自己找了知乎日报的API来测试学习。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/iKrelve/KuaiHu/blob/master/%E7%9F%A5%E4%B9%8E%E6%97%A5%E6%8A%A5API.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;知乎日报API&lt;/a&gt;：&lt;a href=&quot;https://github.com/iKrelve/KuaiHu/blob/master/%E7%9F%A5%E4%B9%8E%E6%97%A5%E6%8A%A5API.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/iKrelve/KuaiHu/blob/master/%E7%9F%A5%E4%B9%8E%E6%97%A5%E6%8A%A5API.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;然后发现大神也使用了这个库，瞬间感觉这个库还是很6的。&lt;/p&gt;
&lt;p&gt;看了学习链接的知识，应该知道要使用&lt;code&gt;AsyncHttpClient.get()&lt;/code&gt;,里面的参数一个Url，一个是要我们实现的匿名内部类。看上去还是很简单的，用起来就更简单了。&lt;/p&gt;
&lt;p&gt;首先拿一个简单的来试试用法，就拿启动界面图像获取这个Url来测试。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Android开发学习笔记秘籍-二十一&quot;&gt;&lt;a href=&quot;#Android开发学习笔记秘籍-二十一&quot; class=&quot;headerlink&quot; title=&quot;Android开发学习笔记秘籍(二十一)&quot;&gt;&lt;/a&gt;Android开发学习笔记秘籍(二十一)&lt;/h1&gt;&lt;p&gt;
    
    </summary>
    
      <category term="Android开发" scheme="http://chr10003566.github.io./categories/Android%E5%BC%80%E5%8F%91/"/>
    
    
  </entry>
  
</feed>
